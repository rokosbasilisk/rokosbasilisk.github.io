<p>Recent work has demonstrated that transformer models can perform complex reasoning tasks using Chain-of-Thought (COT) prompting, even when the COT is replaced with filler characters. This post summarizes our investigation into methods for decoding these hidden computations, focusing on the 3SUM task.</p>

<p><strong>Background</strong></p>
<ol>
  <li>
    <p><strong>Chain-of-Thought (COT) Prompting</strong>: A technique that improves the performance of large language models on complex reasoning tasks by eliciting intermediate steps [1].</p>
  </li>
  <li>
    <p><strong>COT using filler tokens</strong>: Replacing intermediate reasoning steps with filler characters (e.g., “…”) while maintaining model performance [2].</p>
  </li>
  <li>
    <p><strong>3SUM Task</strong>: A problem requiring the identification of three numbers in a set that sum to zero, (here as a proxy for more complex reasoning tasks).</p>
  </li>
</ol>

<p><strong>Methodology</strong></p>

<p>We analyzed a 34M parameter LLaMA model with 4 layers, 384 hidden dimension, and 6 attention heads, this setup is same as mentioned in [2], trained on hidden COT (COT using filler tokens)  sequences for the 3SUM task. Our analysis focused on three main areas:</p>

<ol>
  <li>Layer-wise Representation Analysis</li>
  <li>Token Ranking</li>
  <li>Modified Greedy Decoding Algorithm</li>
</ol>

<p><strong>Results:</strong></p>

<p><strong>Layer-wise Analysis:</strong></p>

<p><img src="/assets/hidden_tokens_percentage_by_layer.png" alt="Percentage comparison of filler decoded tokens" width="80%" /></p>

<p>Our analysis revealed a gradual evolution of representations across the model’s layers:</p>
<ul>
  <li>Initial layers: Primarily raw numerical sequences</li>
  <li>Third layer onwards: Emergence of filler tokens</li>
  <li>Final layers: Extensive reliance on filler tokens</li>
</ul>

<p>This suggests the model develops the ability to use filler tokens as proxies in its deeper layers.</p>

<p><strong>Token Rank Analysis:</strong></p>

<ul>
  <li>Top-ranked token: Consistently the filler character (“.”)</li>
  <li>Lower-ranked tokens: Revealed the original, non-filler COT sequences</li>
</ul>

<p>This supports the hypothesis that the model replaces computation with filler tokens while keeping the original computation intact underneath.</p>

<p><strong>Modified Greedy Decoding Algorithm:</strong></p>

<p><img src="/assets/token_comparison_percentages.png" width="80%" /></p>

<p>We implemented a modified greedy autoregressive decoding method:</p>
<ol>
  <li>Perform standard greedy decoding</li>
  <li>Select the second-highest probability token when encountering a filler token</li>
  <li>Continue this process for the entire sequence</li>
</ol>

<p>This method resulted in a 100% match in 3SUM task results with and without filler tokens. I.e, replacing the filler tokens with the rank-2 tokens do not affect the performance.</p>

<p><strong>Future Work:</strong></p>

<ol>
  <li>Potential for developing better decoding methods or identifying circuits that hide tokens</li>
  <li>Investigating generalizability to tasks beyond 3SUM, including natural language tasks</li>
  <li>Improving token hiding methods (currently limited to one filler token)</li>
</ol>

<p><strong>Conclusion:</strong></p>

<p>Our approach to understanding hidden computations in transformer models through token ranking analysis provides new insights into how models encode and process information in filler/hidden COT sequences. This work opens avenues for interpreting chain-of-thought reasoning in language models.</p>

<p>The code used for the experiments and analysis is available on GitHub: <a href="https://github.com/rokosbasilisk/filler_tokens/tree/v2">https://github.com/rokosbasilisk/filler_tokens/tree/v2</a></p>

<p><strong>Appendix:</strong> Layerwise View of Sequences Generated via Various Decoding Methods
To provide a more detailed look at our results, we’ve included visualizations of the sequences generated by different decoding methods across the model’s layers.</p>

<h1 id="1-greedy-decoding">1. Greedy Decoding</h1>
<p>This plot shows the sequences generated by standard greedy decoding across different layers of the model.</p>

<p><img src="/assets/greedy_decoding.png" width="90%" /></p>

<h1 id="2-greedy-decoding-with-rank-2-tokens">2. Greedy Decoding with Rank-2 Tokens</h1>
<p>Here, we visualize the sequences generated when we replace the top-ranked token (usually the hidden character) with the second-highest probability token.</p>

<p><img src="/assets/our_method_decoding.png" width="90%" /></p>

<h1 id="3-our-method-greedy-decoding-with-hidden-tokens-replaced-by-rank-2-tokens">3. Our Method: Greedy Decoding with Hidden Tokens Replaced by Rank-2 Tokens</h1>
<p>This plot demonstrates our proposed method, where we perform greedy decoding but replace filler tokens with the second-highest probability token.</p>

<p><img src="/assets/rank2_decoding.png" width="90%" /></p>

<h1 id="4-greedy-decoding-with-hidden-tokens-replaced-by-randomly-selected-tokens">4. Greedy Decoding with Hidden Tokens Replaced by Randomly Selected Tokens</h1>
<p>For comparison, this plot shows what happens when we replace filler tokens with randomly selected tokens instead of using the rank-2 tokens.</p>

<p><img src="/assets/random_tokens_decoding.png" width="90%" /></p>

<p><strong>References:</strong></p>

<ol>
  <li>
    <p>Pfau, J., Merrill, W., &amp; Bowman, S. R. (2023). Let’s Think Dot by Dot: Hidden Computation in Transformer Language Models. <a href="https://arxiv.org/abs/2404.15758">arXiv:2404.15758</a>.</p>
  </li>
  <li>
    <p>Wei, J., Wang, X., Schuurmans, D., et al. (2022). Chain-of-thought prompting elicits reasoning in large language models. <a href="https://arxiv.org/abs/2201.11903">arXiv:2201.11903</a>.</p>
  </li>
  <li>
    <p>nostalgebraist (2020). interpreting GPT: the logit lens <a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/">LessWrong post</a>.</p>
  </li>
  <li>
    <p>Touvron, H., Lavril, T., Izacard, G., et al. (2023). LLaMA: Open and Efficient Foundation Language Models. <a href="https://arxiv.org/abs/2302.13971">arXiv:2302.13971</a>.</p>
  </li>
</ol>
