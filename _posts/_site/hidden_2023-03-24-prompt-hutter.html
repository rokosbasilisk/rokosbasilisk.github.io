<p><a href="https://www.lesswrong.com/posts/axxnpQi8FyBPE4rbq/hutter-prize-for-prompts"><em>link to lesswrong post</em></a></p>

<p>The aim of the <a href="http://www.hutter1.net/">Hutter Prize</a> is to compress the first 1GB of Wikipedia to the smallest possible size. From the AIXI standpoint, compression is equal to AI, and if we can compress this to the ideal size (75MB according to Shannonâ€™s lower estimate), then the compression algorithm is equivalent to AIXI.</p>

<p>However, all the winning solutions so far are based on <a href="http://prize.hutter1.net/hfaq.htm#paq8">arithmetic encoding, context mixing</a>. These solutions hold little relevance in mainstream AGI research.</p>

<p>Current day LLMs are really powerful <a href="https://bmk.sh/2020/08/17/Building-AGI-Using-Language-Models/">models of our world</a>.And it is highly possible that most LLMs are trained on data that consist of the first 1GB of Wikipedia. Therefore, with appropriate prompting, it should be possible to extract most or all of this data from the LLMs.</p>
