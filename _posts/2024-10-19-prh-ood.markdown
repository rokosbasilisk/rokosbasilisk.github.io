---
layout: post
title: "Experiments with the Platonic Representation Hypothesis"
excerpt: "Investigating the validity of PRH in OOD setting"
date:   2024-10-27 10:00:00
mathjax: false
comments: false
---

### Introduction

The [Platonic Representation Hypothesis](https://arxiv.org/pdf/2405.07987) (PRH) suggests that neural networks, despite being trained with different objectives and on various modalities, converge to a shared statistical understanding of reality. Inspired by Plato's Allegory of the Cave, PRH posits that models across domains—such as vision and language—internalize similar structures, reflecting an idealized model of the world. While initial experiments focused on image-based models like Vision Transformers (ViT) trained on the same pretraining dataset (ImageNet), this raises an important question:

### Does PRH hold only when models are trained on data from the same distribution?

### Methodology:

To rigorously evaluate PRH beyond in-distribution data, I extended experiments to encompass a variety of challenging scenarios:

### 1. Representational Alignment Across Data Types:
   - **In-distribution data:** Utilized the validation set from Places365, aligning closely with the training data of many vision models.
   - **Out-of-distribution (OOD) data:** Employed the [ImageNet-O](https://arxiv.org/pdf/1907.07174) dataset, containing images that diverge significantly from the standard ImageNet distribution.
   - **Random noise:** Generated purely random images to probe the extreme boundaries of representational alignment.

   Alignment was measured using Spearman's rank correlation on mutual k-NN distances across model representations, assessing how models interpret the similarity of various data inputs.

### 2. Progressive Noise Injection in Vision Models:
   - Corrupted a set of 250 images with Gaussian noise over 100 incremental steps.
   - Measured mutual alignment across 17 Vision Transformer models at each noise level.
   - Analyzed alignment scores to detect patterns in how noise impacts convergence.

### 3. Tracking Alignment During Language Model Training:
   - Analyzed six large language models (LLMs) using checkpoints saved at 100 different training stages.
   - Measured alignment between every pair of models at each checkpoint to observe changes in representational similarity over time.
   - Aimed to uncover phases of increased or decreased similarity, providing insights into how models internalize structure during training.

### Results:

#### In-Distribution Setting

<img src="/assets/prh_correlation_plot.png" alt="Spearman rank correlation between models on Places365's validation data" style="width:100%; height:auto;">

*Figure 1: Spearman rank correlation between models on Places365's validation data, indicating a high degree of alignment in in-distribution data.* This graph is straight from the platonic-representation-hypothesis paper. The ViT models used in this experiment are trained more or less on the same data distribution/pretraining datasets. 

#### Out-of-Distribution Data

<img src="/assets/spearman_correlation_ood_prh.png" alt="Alignment of vision models on the ImageNet-O dataset" style="width:100%; height:auto;">

*Figure 2: Alignment of vision models on the ImageNet-O dataset. Spearman correlation suggests that even with OOD data, models maintain a shared statistical representation, models converge in their representations, implying PRH still holds true and generalizes well outside the training distribution of the model's training set. This fact has really good implications in understanding failure models and alignment failures of models belonging to same architectural family

#### Random Noise Data

<img src="/assets/prh_correlation_rnd.png" alt="Spearman correlation of alignment scores on random noise data" style="width:80%; height:auto; display: block; margin-left: auto; margin-right: auto;">

*Figure 3: Spearman correlation of alignment scores on random noise data. The lower correlation values indicate a breakdown in representational alignment, suggesting that random data lacks the underlying structure required for PRH.* This means that PRH is not just a random occurance but requires some statistical-structure to be present.

### Discussion

When I first came across the PRH paper, my initial hypothesis was that PRH might be primarily due to the high similarity in training datasets, since most LLMs/LVMs are trained on extremely large datasets from common sources (ImageNet, Pile, etc.). However, the OOD experiments proved that this phenomenon of shared statistical structure generalizes well beyond the training distribution of the models. Further experiments with purely random data demonstrated that this is not just a coincidence but rather reflects inherent statistical structures present in "useful" data across all modalities.

The code for experiments and analysis in this paper is available on GitHub [here](https://github.com/rokosbasilisk/prh-experiments).
