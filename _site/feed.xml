<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ram Bharadwaj</title>
    <description>cached thoughts</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Tue, 13 Aug 2024 02:44:25 +0530</pubDate>
    <lastBuildDate>Tue, 13 Aug 2024 02:44:25 +0530</lastBuildDate>
    <generator>Jekyll v3.8.6</generator>
    
      <item>
        <title>Indian Philosophies as Reinforcement Learning</title>
        <description>&lt;p&gt;&lt;strong&gt;Indian Philosophies as Reinforcement Learning&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Reinforcement Learning (RL) provides a powerful framework for understanding intelligence and decision-making. At its core, RL models an agent interacting with an environment, taking actions to maximize cumulative reward. This paradigm turns out to be surprisingly applicable to various domains, including, as I’ve been pondering lately, Indian philosophical systems.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The RL Setup&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let’s start with a quick refresher on RL. The basic setup involves:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;An agent&lt;/li&gt;
  &lt;li&gt;An environment&lt;/li&gt;
  &lt;li&gt;A set of possible actions&lt;/li&gt;
  &lt;li&gt;State observations&lt;/li&gt;
  &lt;li&gt;A reward signal&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The agent’s goal is to learn a policy that maximizes expected cumulative reward over time. This framework is remarkably general and can model a wide range of situations.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/agentinterface.png&quot; alt=&quot;*Agent Environment Interface*&quot; /&gt;
&lt;br /&gt;
&lt;em&gt;The Agent-Environment Interface in Reinforcement Learning&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Indian Philosophy: The Big Picture&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Now, let’s turn to Indian philosophy. While diverse, many schools share some common themes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Atman: Individual consciousness&lt;/li&gt;
  &lt;li&gt;Brahman: Supreme or universal consciousness&lt;/li&gt;
  &lt;li&gt;Samsara: The phenomenal world of experience and rebirth&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The central question often revolves around the relationship between these entities. How does individual consciousness relate to the universal? What’s the nature of our experienced reality?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;An RL Perspective on Advaita Vedanta&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let’s start with Advaita Vedanta, one of the most influential schools of Indian philosophy. In the RL framework, we might map it like this:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Atman (individual consciousness) → Agent&lt;/li&gt;
  &lt;li&gt;Samsara (phenomenal world) → Environment&lt;/li&gt;
  &lt;li&gt;Brahman (supreme consciousness) → The “true” nature of reality&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/sankara.jpg&quot; alt=&quot;*Adi Sankara*&quot; /&gt;
&lt;br /&gt;
&lt;em&gt;Adi Shankara, the propounder of Advaita Vedanta&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The key insight of Advaita is that Atman and Brahman are ultimately identical. In RL terms, this is like saying the agent and the entirety of the system (agent + environment) are one and the same. The apparent duality – the sense of being a separate agent in an environment – is considered an illusion (maya). It’s as if the universal consciousness is playing a cosmic game of RL with itself, creating the appearance of separate agents and an environment.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Visistadvaita: Qualified Non-Dualism&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Moving on to Visistadvaita, proposed by Ramanuja, we can model it as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Atman (individual consciousness) → Agent&lt;/li&gt;
  &lt;li&gt;Samsara (phenomenal world) → Environment&lt;/li&gt;
  &lt;li&gt;Brahman (supreme consciousness) → A specific, named entity (e.g., Vishnu)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ramanuja.jpg&quot; alt=&quot;*Ramanujacharya*&quot; /&gt;
&lt;br /&gt;
&lt;em&gt;Ramanujacharya, the propounder of Visistadvaita&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In this view, the multiplicity of agents and the environment occurs due to “ego” - a failure to cooperate with other agents while being misled by small rewards within the environment. It’s like an RL scenario where agents are optimizing for local rewards, missing the bigger picture of global optimization. The key difference from Advaita is that while Atman is part of Brahman, it maintains some level of distinctness. In RL terms, it’s as if the agents are sub-components of a larger system, each with their own local reward functions, but ultimately part of a greater whole.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Dvaita: Dualism&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Dvaita, proposed by Madhva, takes a different approach:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Atman (individual consciousness) → Distinct agent&lt;/li&gt;
  &lt;li&gt;Samsara (phenomenal world) → Distinct environment&lt;/li&gt;
  &lt;li&gt;Brahman (supreme consciousness) → Separate supreme agent creating the environment&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/madhva.jpg&quot; alt=&quot;*Madhvacharya*&quot; /&gt; 
&lt;em&gt;Madhvacharya, the propounder of Dvaita&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In this view, the agent, environment, and supreme agent are indeed separate. It’s akin to a multi-agent RL system where one “super-agent” (Brahman) creates and maintains the environment for other agents (Atman) to operate in. This perspective aligns well with hierarchical RL frameworks, where higher-level agents can shape the environment or reward structures for lower-level agents.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Buddhism: A Different Approach&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Buddhism, while not typically categorized with the above systems, offers an interesting contrast:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Rejects the existence of a supreme agent (no Brahman)&lt;/li&gt;
  &lt;li&gt;Focuses on the agent’s experience and decision-making process&lt;/li&gt;
  &lt;li&gt;Posits that the optimal policy is to detach from the reward function itself&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/buddha.jpg&quot; alt=&quot;*Gautama Buddha*&quot; /&gt;
&lt;br /&gt;
&lt;em&gt;Gautama Buddha, the founder of Buddhism&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In RL terms, Buddhism suggests that the root of suffering (suboptimal outcomes) lies in the reward function itself. The path to enlightenment could be seen as learning to operate without being driven by the conventional reward signal. This is a fascinating inversion of the standard RL objective. Instead of maximizing cumulative reward, the goal becomes to transcend the very notion of reward-seeking behavior.&lt;/p&gt;
</description>
        <pubDate>Mon, 12 Aug 2024 05:32:10 +0530</pubDate>
        <link>http://localhost:4000/2024/08/12/rlphilosphy/</link>
        <guid isPermaLink="true">http://localhost:4000/2024/08/12/rlphilosphy/</guid>
        
        
      </item>
    
      <item>
        <title>Decrypting hidden chain of thought</title>
        <description>&lt;p&gt;The paper &lt;a href=&quot;https://arxiv.org/abs/2404.15758&quot;&gt;&lt;em&gt;“Let’s Think Dot by Dot”&lt;/em&gt;&lt;/a&gt; introduces an intriguing approach to Chain of Thought (CoT) reasoning in language models. The authors trained a small 34M LLaMA model on CoT sequences for 3SUM and 2SUM tasks, where the sequences were replaced with hidden characters (“.”). Notably, the model performs well on the 3SUM task with the hidden CoT but fails without it.&lt;/p&gt;

&lt;p&gt;While the explicit training on these hidden sequences may seem unnatural, it raises an important question: Can we decrypt and understand what’s happening “under the hood” during the hidden character CoT process?&lt;/p&gt;

&lt;p&gt;The significant performance drop observed without the hidden CoT suggests that meaningful computations are occurring beneath the surface. To investigate this phenomenon, I conducted some small experiments aimed at answering two key questions:&lt;/p&gt;

&lt;p&gt;What information is contained in the logits during this chain-of-thought?
How does the hidden CoT information propagate through the model’s layers?&lt;/p&gt;

&lt;p&gt;Initial findings:&lt;/p&gt;

&lt;p&gt;During greedy decoding, while the top-ranked token is always a hidden character, lower-ranked tokens (rank 2 and below) contain sequences without hidden information. This suggests that the model maintains access to the original, unhidden information throughout the process.&lt;/p&gt;

&lt;p&gt;Using a logit-lens-like method, I observed that the initial layers still contain the pure number sequences related to the 3SUM’s CoT. These are gradually replaced with hidden characters in subsequent layers.
These preliminary results indicate that the model retains and processes the original information, even while outputting hidden characters. Further experiments are needed to gain additional insights into this intriguing phenomenon.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/hiddencot.png&quot; alt=&quot;*Hidden COT decoded tokens*&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Future work: Conduct more in-depth analyses of the model’s internal representations and information flow to better understand the mechanisms underlying the hidden CoT process.&lt;/p&gt;
</description>
        <pubDate>Sun, 11 Aug 2024 07:19:00 +0530</pubDate>
        <link>http://localhost:4000/2024/08/11/hiddencot/</link>
        <guid isPermaLink="true">http://localhost:4000/2024/08/11/hiddencot/</guid>
        
        
      </item>
    
      <item>
        <title>Goal-misgeneralization is ELK-hard</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://www.lesswrong.com/posts/MWSCqzPrAbNrYoqWv/goal-misgeneralization-is-elk-hard&quot;&gt;&lt;em&gt;link to lesswrong post&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Consider an adversarial training-scheme for solving goal-misgeneralization, ( here i consider Redwood Research’s work on &lt;a href=&quot;https://arxiv.org/pdf/2205.01663.pdf&quot;&gt;“Adversarial Training for High-Stakes Reliability”&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Consider a model that was trained to perform a specific task. To guarantee worst-case performance for this model, we need to have bounds for its outputs in adversarial examples. However, it is not feasible to run the model in the actual environment because it could lead to unwanted consequences.&lt;/p&gt;

&lt;p&gt;Therefore, we can use a prediction-head to generate a simulated version of the trajectory that the model might undergo. We can train a classifier from the output of the prediction-head to classify whether the AI system exhibits goal-misgeneralization in an adversarial manner. The model can then be penalized for misgeneralization behavior, and the policy obtained can be distilled into a newer model iteratively. We can use this prediction-head to build another model that can act as a classifier and distill the conservative policy (model) into a newer one.&lt;/p&gt;

&lt;p&gt;However, this is only possible when the prediction-head is not deceiving, which is essentially the same as the &lt;a href=&quot;https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/&quot;&gt;ELK problem&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Fri, 24 Mar 2023 15:30:00 +0530</pubDate>
        <link>http://localhost:4000/2023/03/24/goalmisgen/</link>
        <guid isPermaLink="true">http://localhost:4000/2023/03/24/goalmisgen/</guid>
        
        
      </item>
    
      <item>
        <title>Hutter-Prize for Prompts</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://www.lesswrong.com/posts/axxnpQi8FyBPE4rbq/hutter-prize-for-prompts&quot;&gt;&lt;em&gt;link to lesswrong post&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The aim of the &lt;a href=&quot;http://www.hutter1.net/&quot;&gt;Hutter Prize&lt;/a&gt; is to compress the first 1GB of Wikipedia to the smallest possible size. From the AIXI standpoint, compression is equal to AI, and if we can compress this to the ideal size (75MB according to Shannon’s lower estimate), then the compression algorithm is equivalent to AIXI.&lt;/p&gt;

&lt;p&gt;However, all the winning solutions so far are based on &lt;a href=&quot;http://prize.hutter1.net/hfaq.htm#paq8&quot;&gt;arithmetic encoding, context mixing&lt;/a&gt;. These solutions hold little relevance in mainstream AGI research.&lt;/p&gt;

&lt;p&gt;Current day LLMs are really powerful &lt;a href=&quot;https://bmk.sh/2020/08/17/Building-AGI-Using-Language-Models/&quot;&gt;models of our world&lt;/a&gt;.And it is highly possible that most LLMs are trained on data that consist of the first 1GB of Wikipedia. Therefore, with appropriate prompting, it should be possible to extract most or all of this data from the LLMs.&lt;/p&gt;
</description>
        <pubDate>Fri, 24 Mar 2023 15:30:00 +0530</pubDate>
        <link>http://localhost:4000/2023/03/24/prompt-hutter/</link>
        <guid isPermaLink="true">http://localhost:4000/2023/03/24/prompt-hutter/</guid>
        
        
      </item>
    
      <item>
        <title>The AGI needs to be honest</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://www.lesswrong.com/posts/hqzHbew35Jx4xoDhE/the-agi-needs-to-be-honest&quot;&gt;&lt;em&gt;link to lesswrong post&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Imagine that you are a trained mathematician and you have been assigned the job of testing an arbitrarily intelligent chatbot for its intelligence.&lt;/p&gt;

&lt;p&gt;You being knowledgeable about a fair amount of computer-science theory won’t test it with the likes of Turing-test or similar, since such a bot might have any useful priors about the world.&lt;/p&gt;

&lt;p&gt;You have asked it find a proof for the “Riemann hypothesis”. the bot started its search program and after several months it gave you gigantic proof written in coq.&lt;/p&gt;

&lt;p&gt;You have tried to run the proof through a proof-checking assistant but quickly realized that checking that itself would years or decades, also no other computer except the one running the bot is sophisticated enough to run such a gigantic proof.&lt;/p&gt;

&lt;p&gt;You have asked the bot to provide you a zero-knowledge-proof, but being a trained mathematician you know that a zero-knowledge-proof of sufficient credibility requires as much compute as the original one. also, the correctness is directly linked to the length of the proof it generates.&lt;/p&gt;

&lt;p&gt;You know that the bot may have formed increasingly complex abstractions while solving the problem, and it would be very hard to describe those in exact language to you.&lt;/p&gt;

&lt;p&gt;You have asked the bot to summarize the proof for you in natural-language, but you know that the bot can easily trick you into accepting the proof.&lt;/p&gt;

&lt;p&gt;You have now started to think about a bigger question, the bot essentially is a powerful optimizer. In this case, the bot is trained to find proofs, its reward is based on finding what a group of mathematicians agree on how a correct proof looks like.&lt;/p&gt;

&lt;p&gt;But the bot being bot doesn’t care about being true to you or itself, it is not rewarded for being “honest” it is only being rewarded for finding proof-like strings that humans may select or reject.&lt;/p&gt;

&lt;p&gt;So it is far easier to find a large coq-program, large enough that you cannot check by any other means than to find a proof for Riemann-hypothesis.&lt;/p&gt;

&lt;p&gt;Now you have concluded that before you certify that the bot is intelligent, you have to prove that the bot is being honest.&lt;/p&gt;

&lt;p&gt;Going by the current trend, it is okay for us to assume that such an arbitrarily intelligent bot would have a significant part of it based on the principles of the current deep-learning stack. assume it be a large neural-network-based agent, also assume that the language-understanding component is somewhat based on the current language-model design.&lt;/p&gt;

&lt;p&gt;So how do you know that the large language model is being honest?&lt;/p&gt;

&lt;p&gt;A quick look at the plots of results on truthful-qa dataset shows that truthfulness reduces with the model-size, going by this momentum any large-models trained on large datasets are more likely to give fluke answers to significantly complex questions.
&lt;img src=&quot;/assets/truthfulqa.png&quot; alt=&quot;*TruthfulQA benchmark*&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Any significantly complex decision-question if cast into an optimization problem has one hard-to-find global-minima called “truth” but an extremely large number of easy-to-find local-minima, how do you then make a powerful optimizer optimize for honesty?&lt;/p&gt;

</description>
        <pubDate>Sat, 16 Oct 2021 15:30:00 +0530</pubDate>
        <link>http://localhost:4000/2021/10/16/truth/</link>
        <guid isPermaLink="true">http://localhost:4000/2021/10/16/truth/</guid>
        
        
      </item>
    
  </channel>
</rss>
