<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ram Bharadwaj</title>
    <description>cached thoughts</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Wed, 30 Oct 2024 23:39:29 +0530</pubDate>
    <lastBuildDate>Wed, 30 Oct 2024 23:39:29 +0530</lastBuildDate>
    <generator>Jekyll v3.8.6</generator>
    
      <item>
        <title>Exploring the Platonic Representation Hypothesis Beyond In-Distribution Data</title>
        <description>&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://arxiv.org/pdf/2405.07987&quot;&gt;Platonic Representation Hypothesis&lt;/a&gt; (PRH) suggests that neural networks, despite being trained with different objectives and on various modalities, converge to a shared statistical understanding of reality. Inspired by Plato’s Allegory of the Cave, PRH posits that models across domains—such as vision and language—internalize similar structures, reflecting an idealized model of the world. While initial experiments focused on image-based models like Vision Transformers (ViT) trained on the same pretraining dataset (ImageNet), this raises an important question:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Does PRH hold only when models are trained on data from the same distribution?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Methodology&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To rigorously evaluate PRH beyond in-distribution data, we extended our experiments to encompass a variety of challenging scenarios:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Representational Alignment Across Data Types:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;In-distribution data:&lt;/strong&gt; Utilized the validation set from Places365, aligning closely with the training data of many vision models.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Out-of-distribution (OOD) data:&lt;/strong&gt; Employed the &lt;a href=&quot;https://arxiv.org/pdf/1907.07174&quot;&gt;ImageNet-O&lt;/a&gt; dataset, containing images that diverge significantly from the standard ImageNet distribution.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Random noise:&lt;/strong&gt; Generated purely random images to probe the extreme boundaries of representational alignment.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;Alignment was measured using Spearman’s rank correlation on mutual k-NN distances across model representations, assessing how models interpret the similarity of various data inputs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Progressive Noise Injection in Vision Models:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Corrupted a set of 250 images with Gaussian noise over 100 incremental steps.&lt;/li&gt;
      &lt;li&gt;Measured mutual alignment across 17 Vision Transformer models at each noise level.&lt;/li&gt;
      &lt;li&gt;Analyzed alignment scores to detect patterns in how noise impacts convergence.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tracking Alignment During Language Model Training:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Analyzed six large language models (LLMs) using checkpoints saved at 100 different training stages.&lt;/li&gt;
      &lt;li&gt;Measured alignment between every pair of models at each checkpoint to observe changes in representational similarity over time.&lt;/li&gt;
      &lt;li&gt;Aimed to uncover phases of increased or decreased similarity, providing insights into how models internalize structure during training.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;alignment-across-data-types&quot;&gt;Alignment Across Data Types&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/prh_correlation_plot.png&quot; alt=&quot;Spearman rank correlation between models on Places365's validation data&quot; style=&quot;width:100%; height:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1: Spearman rank correlation between models on Places365’s validation data, indicating a high degree of alignment in in-distribution data.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;out-of-distribution-data&quot;&gt;Out-of-Distribution Data&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/spearman_correlation_ood_prh.png&quot; alt=&quot;Alignment of vision models on the ImageNet-O dataset&quot; style=&quot;width:100%; height:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 2: Alignment of vision models on the ImageNet-O dataset. Spearman correlation suggests that even with outlier data, models maintain a shared statistical representation. Despite prediction errors in this OOD case, models converge in their representations, implying that larger models exhibit predictable error patterns similar to smaller models.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;random-noise-data&quot;&gt;Random Noise Data&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/prh_correlation_rnd.png&quot; alt=&quot;Spearman correlation of alignment scores on random noise data&quot; style=&quot;width:80%; height:auto; display: block; margin-left: auto; margin-right: auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 3: Spearman correlation of alignment scores on random noise data. The lower correlation values indicate a breakdown in representational alignment, suggesting that random data lacks the underlying structure required for PRH.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;impact-of-noise-injection&quot;&gt;Impact of Noise Injection&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/alignment_vs_signal_level.png&quot; alt=&quot;Relationship between noise level and mean alignment scores of 17 ViT models&quot; style=&quot;width:100%; height:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 4: Relationship between noise level and mean alignment scores of 17 ViT models. A quadratic trend is observed as noise increases, suggesting a threshold of signal required for representational alignment to occur.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/alignment_plot.png&quot; alt=&quot;Alignment of individual ViT models with respect to noise level&quot; style=&quot;width:80%; height:auto; display: block; margin-left: auto; margin-right: auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 5: Alignment of individual ViT models with respect to noise level, highlighting both similarities and differences across the 17 models.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/legend_models.png&quot; alt=&quot;Legend showing the names of individual ViT models used in the study&quot; style=&quot;width:60%; height:auto; display: block; margin-left: auto; margin-right: auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 6: Legend showing the names of individual ViT models used in the study.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;alignment-dynamics-during-llm-training&quot;&gt;Alignment Dynamics During LLM Training&lt;/h3&gt;

&lt;div style=&quot;display: flex; flex-wrap: wrap; gap: 20px; justify-content: center;&quot;&gt;
  &lt;div style=&quot;flex: 1 1 45%; max-width: 45%;&quot;&gt;
    &lt;img src=&quot;/assets/mean_alignment_score_14m.png&quot; alt=&quot;Alignment trends of LLMs during training (14M)&quot; style=&quot;width:100%; height:auto;&quot; /&gt;
  &lt;/div&gt;
  &lt;div style=&quot;flex: 1 1 45%; max-width: 45%;&quot;&gt;
    &lt;img src=&quot;/assets/mean_alignment_score_160m.png&quot; alt=&quot;Alignment trends of LLMs during training (160M)&quot; style=&quot;width:100%; height:auto;&quot; /&gt;
  &lt;/div&gt;
  &lt;div style=&quot;flex: 1 1 45%; max-width: 45%;&quot;&gt;
    &lt;img src=&quot;/assets/mean_alignment_score_410m.png&quot; alt=&quot;Alignment trends of LLMs during training (410M)&quot; style=&quot;width:100%; height:auto;&quot; /&gt;
  &lt;/div&gt;
  &lt;div style=&quot;flex: 1 1 45%; max-width: 45%;&quot;&gt;
    &lt;img src=&quot;/assets/mean_alignment_score_1b.png&quot; alt=&quot;Alignment trends of LLMs during training (1B)&quot; style=&quot;width:100%; height:auto;&quot; /&gt;
  &lt;/div&gt;
  &lt;div style=&quot;flex: 1 1 45%; max-width: 45%;&quot;&gt;
    &lt;img src=&quot;/assets/mean_alignment_score_1.4b.png&quot; alt=&quot;Alignment trends of LLMs during training (1.4B)&quot; style=&quot;width:100%; height:auto;&quot; /&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Figure 7: Alignment trends of LLMs during training. Scores show non-linear trends across different parameter sizes, with initial similarity between smaller and larger models due to similar initialization methods, followed by divergence as larger models form more detailed representations.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Our comprehensive experiments extend the understanding of PRH by demonstrating that neural networks maintain representational alignment even in out-of-distribution (OOD) contexts. However, this alignment breaks down when models are exposed to purely random data, indicating that some underlying structure within the dataset is crucial for PRH to hold.&lt;/p&gt;

&lt;p&gt;The noise injection experiments reveal that representational alignment degrades smoothly as noise increases, following a quadratic trend relative to the signal level. This suggests that a certain threshold of information is necessary for models to maintain a shared representation.&lt;/p&gt;

&lt;p&gt;In the realm of language models, alignment dynamics during training show non-linear trends. Initially, smaller and larger models exhibit similar alignment due to shared initialization methods. As training progresses, larger models develop more nuanced and detailed representations, leading to divergence in alignment scores. This highlights the influence of model capacity and training progression on representational convergence.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Our study provides robust support for the Platonic Representation Hypothesis in structured and diverse data environments. The findings underscore the importance of data structure in achieving representational alignment across different models and modalities. While PRH holds in both in-distribution and out-of-distribution settings, it fails in the absence of structured input, as evidenced by random noise data.&lt;/p&gt;

&lt;p&gt;Future research directions include exploring PRH in multimodal settings and investigating how different training strategies impact representational convergence. Understanding these dynamics can further illuminate the shared structures that underpin neural network representations, contributing to advancements in AI alignment and interpretability.&lt;/p&gt;

&lt;p&gt;The code for experiments and analysis in this paper is available on GitHub &lt;a href=&quot;https://github.com/rokosbasilisk/prh-experiments&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Sun, 27 Oct 2024 15:30:00 +0530</pubDate>
        <link>http://localhost:4000/2024/10/27/prh-ood/</link>
        <guid isPermaLink="true">http://localhost:4000/2024/10/27/prh-ood/</guid>
        
        
      </item>
    
      <item>
        <title>Understanding Hidden Computations in Chain-of-Thought Reasoning</title>
        <description>&lt;p&gt;Recent work has demonstrated that transformer models can perform complex reasoning tasks using Chain-of-Thought (COT) prompting, even when the COT is replaced with filler characters. This post summarizes our investigation into methods for decoding these hidden computations, focusing on the 3SUM task.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Chain-of-Thought (COT) Prompting&lt;/strong&gt;: A technique that improves the performance of large language models on complex reasoning tasks by eliciting intermediate steps [1].&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;COT using filler tokens&lt;/strong&gt;: Replacing intermediate reasoning steps with filler characters (e.g., “…”) while maintaining model performance [2].&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;3SUM Task&lt;/strong&gt;: A problem requiring the identification of three numbers in a set that sum to zero, (here as a proxy for more complex reasoning tasks).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Methodology&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We analyzed a 34M parameter LLaMA model with 4 layers, 384 hidden dimension, and 6 attention heads, this setup is same as mentioned in [2], trained on hidden COT (COT using filler tokens)  sequences for the 3SUM task. Our analysis focused on three main areas:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Layer-wise Representation Analysis&lt;/li&gt;
  &lt;li&gt;Token Ranking&lt;/li&gt;
  &lt;li&gt;Modified Greedy Decoding Algorithm&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Layer-wise Analysis:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/hidden_tokens_percentage_by_layer.png&quot; alt=&quot;Percentage comparison of filler decoded tokens&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our analysis revealed a gradual evolution of representations across the model’s layers:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Initial layers: Primarily raw numerical sequences&lt;/li&gt;
  &lt;li&gt;Third layer onwards: Emergence of filler tokens&lt;/li&gt;
  &lt;li&gt;Final layers: Extensive reliance on filler tokens&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This suggests the model develops the ability to use filler tokens as proxies in its deeper layers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Token Rank Analysis:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Top-ranked token: Consistently the filler character (“.”)&lt;/li&gt;
  &lt;li&gt;Lower-ranked tokens: Revealed the original, non-filler COT sequences&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This supports the hypothesis that the model replaces computation with filler tokens while keeping the original computation intact underneath.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Modified Greedy Decoding Algorithm:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/token_comparison_percentages.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We implemented a modified greedy autoregressive decoding method:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Perform standard greedy decoding&lt;/li&gt;
  &lt;li&gt;Select the second-highest probability token when encountering a filler token&lt;/li&gt;
  &lt;li&gt;Continue this process for the entire sequence&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This method resulted in a 100% match in 3SUM task results with and without filler tokens. I.e, replacing the filler tokens with the rank-2 tokens do not affect the performance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Future Work:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Potential for developing better decoding methods or identifying circuits that hide tokens&lt;/li&gt;
  &lt;li&gt;Investigating generalizability to tasks beyond 3SUM, including natural language tasks&lt;/li&gt;
  &lt;li&gt;Improving token hiding methods (currently limited to one filler token)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Conclusion:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Our approach to understanding hidden computations in transformer models through token ranking analysis provides new insights into how models encode and process information in filler/hidden COT sequences. This work opens avenues for interpreting chain-of-thought reasoning in language models.&lt;/p&gt;

&lt;p&gt;The code used for the experiments and analysis is available on GitHub: &lt;a href=&quot;https://github.com/rokosbasilisk/filler_tokens/tree/v2&quot;&gt;https://github.com/rokosbasilisk/filler_tokens/tree/v2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Appendix:&lt;/strong&gt; Layerwise View of Sequences Generated via Various Decoding Methods
To provide a more detailed look at our results, we’ve included visualizations of the sequences generated by different decoding methods across the model’s layers.&lt;/p&gt;

&lt;h1 id=&quot;1-greedy-decoding&quot;&gt;1. Greedy Decoding&lt;/h1&gt;
&lt;p&gt;This plot shows the sequences generated by standard greedy decoding across different layers of the model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/greedy_decoding.png&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;2-greedy-decoding-with-rank-2-tokens&quot;&gt;2. Greedy Decoding with Rank-2 Tokens&lt;/h1&gt;
&lt;p&gt;Here, we visualize the sequences generated when we replace the top-ranked token (usually the hidden character) with the second-highest probability token.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/our_method_decoding.png&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;3-our-method-greedy-decoding-with-hidden-tokens-replaced-by-rank-2-tokens&quot;&gt;3. Our Method: Greedy Decoding with Hidden Tokens Replaced by Rank-2 Tokens&lt;/h1&gt;
&lt;p&gt;This plot demonstrates our proposed method, where we perform greedy decoding but replace filler tokens with the second-highest probability token.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/rank2_decoding.png&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;4-greedy-decoding-with-hidden-tokens-replaced-by-randomly-selected-tokens&quot;&gt;4. Greedy Decoding with Hidden Tokens Replaced by Randomly Selected Tokens&lt;/h1&gt;
&lt;p&gt;For comparison, this plot shows what happens when we replace filler tokens with randomly selected tokens instead of using the rank-2 tokens.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/random_tokens_decoding.png&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Pfau, J., Merrill, W., &amp;amp; Bowman, S. R. (2023). Let’s Think Dot by Dot: Hidden Computation in Transformer Language Models. &lt;a href=&quot;https://arxiv.org/abs/2404.15758&quot;&gt;arXiv:2404.15758&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Wei, J., Wang, X., Schuurmans, D., et al. (2022). Chain-of-thought prompting elicits reasoning in large language models. &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;&gt;arXiv:2201.11903&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;nostalgebraist (2020). interpreting GPT: the logit lens &lt;a href=&quot;https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/&quot;&gt;LessWrong post&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Touvron, H., Lavril, T., Izacard, G., et al. (2023). LLaMA: Open and Efficient Foundation Language Models. &lt;a href=&quot;https://arxiv.org/abs/2302.13971&quot;&gt;arXiv:2302.13971&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 28 Aug 2024 07:19:00 +0530</pubDate>
        <link>http://localhost:4000/2024/08/28/fillertokens/</link>
        <guid isPermaLink="true">http://localhost:4000/2024/08/28/fillertokens/</guid>
        
        
      </item>
    
      <item>
        <title>Indian Philosophies as Reinforcement Learning</title>
        <description>&lt;p&gt;&lt;strong&gt;Indian Philosophies as Reinforcement Learning&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Reinforcement Learning (RL) provides a powerful framework for understanding intelligence and decision-making. At its core, RL models an agent interacting with an environment, taking actions to maximize cumulative reward. This paradigm turns out to be surprisingly applicable to various domains, including, as I’ve been pondering lately, Indian philosophical systems.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The RL Setup&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let’s start with a quick refresher on RL. The basic setup involves:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;An agent&lt;/li&gt;
  &lt;li&gt;An environment&lt;/li&gt;
  &lt;li&gt;A set of possible actions&lt;/li&gt;
  &lt;li&gt;State observations&lt;/li&gt;
  &lt;li&gt;A reward signal&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The agent’s goal is to learn a policy that maximizes expected cumulative reward over time. This framework is remarkably general and can model a wide range of situations.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/agentinterface.png&quot; alt=&quot;*Agent Environment Interface*&quot; /&gt;
&lt;br /&gt;
&lt;em&gt;The Agent-Environment Interface in Reinforcement Learning&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Indian Philosophy: The Big Picture&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Now, let’s turn to Indian philosophy. While diverse, many schools share some common themes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Atman: Individual consciousness&lt;/li&gt;
  &lt;li&gt;Brahman: Supreme or universal consciousness&lt;/li&gt;
  &lt;li&gt;Samsara: The phenomenal world of experience and rebirth&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The central question often revolves around the relationship between these entities. How does individual consciousness relate to the universal? What’s the nature of our experienced reality?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;An RL Perspective on Advaita Vedanta&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let’s start with Advaita Vedanta, one of the most influential schools of Indian philosophy. In the RL framework, we might map it like this:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Atman (individual consciousness) → Agent&lt;/li&gt;
  &lt;li&gt;Samsara (phenomenal world) → Environment&lt;/li&gt;
  &lt;li&gt;Brahman (supreme consciousness) → The “true” nature of reality&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/sankara.jpg&quot; alt=&quot;*Adi Sankara*&quot; /&gt;
&lt;br /&gt;
&lt;em&gt;Adi Shankara, the propounder of Advaita Vedanta&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The key insight of Advaita is that Atman and Brahman are ultimately identical. In RL terms, this is like saying the agent and the entirety of the system (agent + environment) are one and the same. The apparent duality – the sense of being a separate agent in an environment – is considered an illusion (maya). It’s as if the universal consciousness is playing a cosmic game of RL with itself, creating the appearance of separate agents and an environment.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Visistadvaita: Qualified Non-Dualism&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Moving on to Visistadvaita, proposed by Ramanuja, we can model it as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Atman (individual consciousness) → Agent&lt;/li&gt;
  &lt;li&gt;Samsara (phenomenal world) → Environment&lt;/li&gt;
  &lt;li&gt;Brahman (supreme consciousness) → A specific, named entity (e.g., Vishnu)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ramanuja.jpg&quot; alt=&quot;*Ramanujacharya*&quot; /&gt;
&lt;br /&gt;
&lt;em&gt;Ramanujacharya, the propounder of Visistadvaita&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In this view, the multiplicity of agents and the environment occurs due to “ego” - a failure to cooperate with other agents while being misled by small rewards within the environment. It’s like an RL scenario where agents are optimizing for local rewards, missing the bigger picture of global optimization. The key difference from Advaita is that while Atman is part of Brahman, it maintains some level of distinctness. In RL terms, it’s as if the agents are sub-components of a larger system, each with their own local reward functions, but ultimately part of a greater whole.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Dvaita: Dualism&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Dvaita, proposed by Madhva, takes a different approach:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Atman (individual consciousness) → Distinct agent&lt;/li&gt;
  &lt;li&gt;Samsara (phenomenal world) → Distinct environment&lt;/li&gt;
  &lt;li&gt;Brahman (supreme consciousness) → Separate supreme agent creating the environment&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/madhva.jpg&quot; alt=&quot;*Madhvacharya*&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Madhvacharya, the propounder of Dvaita&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In this view, the agent, environment, and supreme agent are indeed separate. It’s akin to a multi-agent RL system where one “super-agent” (Brahman) creates and maintains the environment for other agents (Atman) to operate in. This perspective aligns well with hierarchical RL frameworks, where higher-level agents can shape the environment or reward structures for lower-level agents.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Buddhism: A Different Approach&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Buddhism, while not typically categorized with the above systems, offers an interesting contrast:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Rejects the existence of a supreme agent (no Brahman)&lt;/li&gt;
  &lt;li&gt;Focuses on the agent’s experience and decision-making process&lt;/li&gt;
  &lt;li&gt;Posits that the optimal policy is to detach from the reward function itself&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/buddha.jpg&quot; alt=&quot;*Gautama Buddha*&quot; /&gt;
&lt;br /&gt;
&lt;em&gt;Gautama Buddha, the founder of Buddhism&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In RL terms, Buddhism suggests that the root of suffering (suboptimal outcomes) lies in the reward function itself. The path to enlightenment could be seen as learning to operate without being driven by the conventional reward signal. This is a fascinating inversion of the standard RL objective. Instead of maximizing cumulative reward, the goal becomes to transcend the very notion of reward-seeking behavior.&lt;/p&gt;
</description>
        <pubDate>Mon, 12 Aug 2024 05:32:10 +0530</pubDate>
        <link>http://localhost:4000/2024/08/12/rlphilosphy/</link>
        <guid isPermaLink="true">http://localhost:4000/2024/08/12/rlphilosphy/</guid>
        
        
      </item>
    
      <item>
        <title>Goal-misgeneralization might be ELK-hard</title>
        <description>&lt;p&gt;Goal misgeneralization and the ELK (Eliciting Latent Knowledge) problem are two of the most intriguing yet well-defined sub-problems within the broader AI alignment challenge. In this post, I’ll share my opinion on why solving goal misgeneralization is as challenging as solving the ELK problem.&lt;/p&gt;

&lt;h3 id=&quot;goal-misgeneralization&quot;&gt;Goal Misgeneralization&lt;/h3&gt;

&lt;p&gt;Goal misgeneralization occurs when a reinforcement learning agent retains its capabilities out-of-distribution yet pursues the wrong goal. This misalignment between the intended goal and the agent’s actual behavior can lead to unexpected and potentially harmful outcomes.
A more detailed explanation of this can be found in the &lt;a href=&quot;https://arxiv.org/abs/2105.14111&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;the-elk-problem&quot;&gt;The ELK Problem&lt;/h3&gt;

&lt;p&gt;Alignment Research Center (ARC) provides a simple, informal explanation of the ELK problem:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Suppose we train a model to predict what the future will look like according to cameras and other sensors. We then use planning algorithms to find a sequence of actions that lead to predicted futures that look good to us.&lt;/p&gt;

  &lt;p&gt;But some action sequences could tamper with the cameras so they show happy humans regardless of what’s really happening. More generally, some futures look great on camera but are actually catastrophically bad.&lt;/p&gt;

  &lt;p&gt;In these cases, the prediction model “knows” facts (like “the camera was tampered with”) that are not visible on camera but would change our evaluation of the predicted future if we learned them. How can we train this model to report its latent knowledge of off-screen events?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A more sophisticated explanation of the ELK problem can be found in the &lt;a href=&quot;https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/&quot;&gt;ELK problem document&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As of now, this problem too remains unsolved.&lt;/p&gt;

&lt;h3 id=&quot;why-solving-goal-misgeneralization-is-as-hard-as-solving-elk&quot;&gt;Why Solving Goal Misgeneralization is as Hard as Solving ELK&lt;/h3&gt;

&lt;p&gt;Consider an adversarial training scheme for solving goal misgeneralization, such as Redwood Research’s work on &lt;a href=&quot;https://arxiv.org/pdf/2205.01663.pdf&quot;&gt;“Adversarial Training for High-Stakes Reliability”&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Let’s examine a model trained to perform a specific task. To guarantee worst-case performance for this model, we need to establish bounds for its outputs in adversarial examples. However, running the model in the actual environment is not feasible due to potential unwanted consequences.&lt;/p&gt;

&lt;p&gt;To address this, we can use a prediction-head to generate a simulated version of the trajectory that the model might undergo. We can then train a classifier from the output of the prediction-head to determine whether the AI system exhibits goal misgeneralization in an adversarial manner. The model can be penalized for misgeneralization behavior, and the resulting policy can be iteratively distilled into a newer model.&lt;/p&gt;

&lt;p&gt;This prediction-head can be used to build another model that acts as a classifier, allowing us to distill the conservative policy (model) into a newer one.&lt;/p&gt;

&lt;p&gt;However, this approach is only viable when the prediction-head is not deceiving, which is essentially the same challenge posed by the ELK problem.&lt;/p&gt;

&lt;p&gt;In conclusion, the difficulty of solving goal misgeneralization for the worst case mirrors the core challenge of the ELK problem.&lt;/p&gt;
</description>
        <pubDate>Fri, 24 Mar 2023 15:30:00 +0530</pubDate>
        <link>http://localhost:4000/2023/03/24/goalmisgen/</link>
        <guid isPermaLink="true">http://localhost:4000/2023/03/24/goalmisgen/</guid>
        
        
      </item>
    
      <item>
        <title>The AGI needs to be honest</title>
        <description>&lt;p&gt;Imagine that you are a trained mathematician and you have been assigned the job of testing an arbitrarily intelligent chatbot for its intelligence.&lt;/p&gt;

&lt;p&gt;You being knowledgeable about a fair amount of computer-science theory won’t test it with the likes of Turing-test or similar, since such a bot might have any useful priors about the world.&lt;/p&gt;

&lt;p&gt;You have asked it find a proof for the “Riemann hypothesis”. the bot started its search program and after several months it gave you gigantic proof written in coq.&lt;/p&gt;

&lt;p&gt;You have tried to run the proof through a proof-checking assistant but quickly realized that checking that itself would years or decades, also no other computer except the one running the bot is sophisticated enough to run such a gigantic proof.&lt;/p&gt;

&lt;p&gt;You have asked the bot to provide you a zero-knowledge-proof, but being a trained mathematician you know that a zero-knowledge-proof of sufficient credibility requires as much compute as the original one. also, the correctness is directly linked to the length of the proof it generates.&lt;/p&gt;

&lt;p&gt;You know that the bot may have formed increasingly complex abstractions while solving the problem, and it would be very hard to describe those in exact language to you.&lt;/p&gt;

&lt;p&gt;You have asked the bot to summarize the proof for you in natural-language, but you know that the bot can easily trick you into accepting the proof.&lt;/p&gt;

&lt;p&gt;You have now started to think about a bigger question, the bot essentially is a powerful optimizer. In this case, the bot is trained to find proofs, its reward is based on finding what a group of mathematicians agree on how a correct proof looks like.&lt;/p&gt;

&lt;p&gt;But the bot being bot doesn’t care about being true to you or itself, it is not rewarded for being “honest” it is only being rewarded for finding proof-like strings that humans may select or reject.&lt;/p&gt;

&lt;p&gt;So it is far easier to find a large coq-program, large enough that you cannot check by any other means than to find a proof for Riemann-hypothesis.&lt;/p&gt;

&lt;p&gt;Now you have concluded that before you certify that the bot is intelligent, you have to prove that the bot is being honest.&lt;/p&gt;

&lt;p&gt;Going by the current trend, it is okay for us to assume that such an arbitrarily intelligent bot would have a significant part of it based on the principles of the current deep-learning stack. assume it be a large neural-network-based agent, also assume that the language-understanding component is somewhat based on the current language-model design.&lt;/p&gt;

&lt;p&gt;So how do you know that the large language model is being honest?&lt;/p&gt;

&lt;p&gt;A quick look at the plots of results on truthful-qa dataset shows that truthfulness reduces with the model-size, going by this momentum any large-models trained on large datasets are more likely to give fluke answers to significantly complex questions.
&lt;img src=&quot;/assets/truthfulqa.png&quot; alt=&quot;*TruthfulQA benchmark*&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Any significantly complex decision-question if cast into an optimization problem has one hard-to-find global-minima called “truth” but an extremely large number of easy-to-find local-minima, how do you then make a powerful optimizer optimize for honesty?&lt;/p&gt;

</description>
        <pubDate>Sat, 16 Oct 2021 15:30:00 +0530</pubDate>
        <link>http://localhost:4000/2021/10/16/truth/</link>
        <guid isPermaLink="true">http://localhost:4000/2021/10/16/truth/</guid>
        
        
      </item>
    
  </channel>
</rss>
