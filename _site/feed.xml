<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Full Wrong</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Mon, 28 Jul 2025 10:25:00 +0530</pubDate>
    <lastBuildDate>Mon, 28 Jul 2025 10:25:00 +0530</lastBuildDate>
    <generator>Jekyll v3.9.0</generator>
    
      <item>
        <title>Scaling Laws for LLM-Based Data Compression</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;https://arxiv.org/abs/2001.08361&quot;&gt;scaling laws for neural language models&lt;/a&gt; showed that cross-entropy loss follows a power law in three factors:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Dataset size&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Model parameters&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Training compute&lt;/strong&gt; (steps or epochs)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The lower the cross-entropy loss, the better the model’s next-token prediction. Since prediction and compression are deeply linked (&lt;a href=&quot;https://mindfulmodeler.substack.com/p/the-intricate-link-between-compression&quot;&gt;The Intricate Link Between Compression and Prediction&lt;/a&gt;), a lower loss should translate into better lossless compression. This raises the question:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Do LLMs exhibit clean power-law scaling in compression performance?&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;experiment&quot;&gt;Experiment&lt;/h2&gt;

&lt;p&gt;For the experiment, I considered the Pythia models, which are available at multiple checkpoints and parameter sizes. All Pythia models are trained on the same dataset (The Pile), ensuring a uniform comparison. Following Delétang et al. (2023), The LLM’s predicted probabilities for each token are used to update the symbol table of an arithmetic encoder (a near-optimal entropy coder). I took the first 2048 chunks of the enwik8 dataset each chunk is 2048 bytes and fed them to the LLM to predict the next-token probabilities.&lt;/p&gt;

&lt;p&gt;The other experimental parameters are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Models:&lt;/strong&gt; EleutherAI Pythia family (8-bit quantized): 70 M, 160 M, 410 M, 1 B, 1.4 B parameters&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Checkpoints:&lt;/strong&gt; 1 k, 8 k, 32 k, 128 k, 143 k training steps&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Chunking:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;CHUNK_SIZE:&lt;/strong&gt; 2048 bytes&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;NUM_CHUNKS:&lt;/strong&gt; 2048 per dataset&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Compression pipeline:&lt;/strong&gt;
    &lt;ol&gt;
      &lt;li&gt;Raw bytes → ASCII via &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;chr(b % 128)&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Tokenize (max_length = 1024)&lt;/li&gt;
      &lt;li&gt;Arithmetic coding on LLM probabilities&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;CR&lt;/strong&gt; = compressed_bits / original_bits&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Datasets &amp;amp; Preprocessing:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Text:&lt;/strong&gt; enwik8 (Wikipedia XML)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Image:&lt;/strong&gt; ImageNet-1k validation → 32 × 64 grayscale patches (uint8 bytes)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Speech:&lt;/strong&gt; LibriSpeech “clean” train.100 → 16 kHz PCM → int16 bytes&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;results-text-compression&quot;&gt;Results: Text Compression&lt;/h2&gt;

&lt;div style=&quot;overflow-x:auto;width:100%;&quot;&gt;
&lt;table style=&quot;width:100%;border:1px solid #444;border-collapse:collapse;&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;border:1px solid #444;padding:4px;text-align:left;&quot;&gt;Model&lt;/th&gt;
      &lt;th style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;1 k&lt;/th&gt;
      &lt;th style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;8 k&lt;/th&gt;
      &lt;th style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;32 k&lt;/th&gt;
      &lt;th style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;128 k&lt;/th&gt;
      &lt;th style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;143 k&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;&quot;&gt;pythia-70M&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.22&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.18&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.17&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.17&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.17&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;&quot;&gt;pythia-160M&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.22&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.16&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.15&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.15&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.15&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;&quot;&gt;pythia-410M&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.22&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.15&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.14&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.13&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.13&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;&quot;&gt;pythia-1 B&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.21&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.14&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.13&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.12&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;&quot;&gt;pythia-1.4 B&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.21&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.14&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.12&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.11&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.11&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;h3 id=&quot;text-scaling-law-fit&quot;&gt;Text scaling-law fit&lt;/h3&gt;

&lt;p&gt;We fit a Kaplan-style power law&lt;br /&gt;
\(\mathrm{CR}(P,S)
= a + b\,P^{-\alpha} + c\,S^{-\beta}.\)&lt;br /&gt;
For &lt;strong&gt;text&lt;/strong&gt;, the best-fit coefficients are:&lt;br /&gt;
\(\mathrm{CR}_{\text{text}}
= 0.10 + 60\,P^{-0.38} + 10\,S^{-0.75}.\)&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;compression-on-non-text-modalities&quot;&gt;Compression on Non-Text Modalities&lt;/h2&gt;

&lt;p&gt;The “Language Modeling Is Compression” paper shows that an LLM trained only on text can compress arbitrary byte streams &lt;a href=&quot;https://arxiv.org/abs/2309.10668&quot;&gt;Language Modeling Is Compression&lt;/a&gt;. I applied the same pipeline to ImageNet-1k patches and LibriSpeech audio.&lt;/p&gt;

&lt;h3 id=&quot;image-results&quot;&gt;Image Results&lt;/h3&gt;

&lt;div style=&quot;overflow-x:auto;width:100%;&quot;&gt;
&lt;table style=&quot;width:100%;border:1px solid #444;border-collapse:collapse;&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;border:1px solid #444;padding:4px;text-align:left;&quot;&gt;Model&lt;/th&gt;
      &lt;th style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;1 k&lt;/th&gt;
      &lt;th style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;8 k&lt;/th&gt;
      &lt;th style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;32 k&lt;/th&gt;
      &lt;th style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;128 k&lt;/th&gt;
      &lt;th style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;143 k&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;&quot;&gt;pythia-70M&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.60&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.50&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.49&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.50&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.51&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;&quot;&gt;pythia-160M&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.61&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.48&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.47&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.48&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.49&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;&quot;&gt;pythia-410M&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.67&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.51&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.46&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.44&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.45&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;&quot;&gt;pythia-1 B&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.60&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.47&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.46&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.44&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.44&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;&quot;&gt;pythia-1.4 B&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.64&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.48&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.47&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.43&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.44&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;h3 id=&quot;image-scaling-law-fit&quot;&gt;Image scaling-law fit&lt;/h3&gt;

\[\mathrm{CR}_{\text{image}}
= 0.38 + 2\,P^{-0.15} + 60\,S^{-0.86}.\]

&lt;h3 id=&quot;speech-results&quot;&gt;Speech Results&lt;/h3&gt;

&lt;div style=&quot;overflow-x:auto;width:100%;&quot;&gt;
&lt;table style=&quot;width:100%;border:1px solid #444;border-collapse:collapse;&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;border:1px solid #444;padding:4px;text-align:left;&quot;&gt;Model&lt;/th&gt;
      &lt;th style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;1 k&lt;/th&gt;
      &lt;th style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;8 k&lt;/th&gt;
      &lt;th style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;32 k&lt;/th&gt;
      &lt;th style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;128 k&lt;/th&gt;
      &lt;th style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;143 k&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;&quot;&gt;pythia-70M&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.69&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.46&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.44&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.47&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.47&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;&quot;&gt;pythia-160M&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.68&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.44&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.43&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.43&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.46&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;&quot;&gt;pythia-410M&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.77&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.51&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.40&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.38&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.39&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;&quot;&gt;pythia-1 B&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.68&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.42&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.44&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.38&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.38&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;&quot;&gt;pythia-1.4 B&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.75&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.47&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.44&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.38&lt;/td&gt;
      &lt;td style=&quot;border:1px solid #444;padding:4px;text-align:right;&quot;&gt;0.39&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;h3 id=&quot;speech-scaling-law-fit&quot;&gt;Speech scaling-law fit&lt;/h3&gt;

\[\mathrm{CR}_{\text{speech}}
= 0.39 + 8\times10^{3}\,P^{-0.68} + 1\times10^{2}\,S^{-0.85}.\]

&lt;hr /&gt;

&lt;h2 id=&quot;combined-scaling-curves&quot;&gt;Combined Scaling Curves&lt;/h2&gt;
&lt;p&gt;The below plot shows the overall scaling laws plots across all the three modalities, While the scaling law trend is present in non-textual modalities, the compression is not as strong in text.&lt;/p&gt;
&lt;div style=&quot;text-align:center;&quot;&gt;
  &lt;img src=&quot;/assets/scaling_laws_for_compression.png&quot; alt=&quot;Scaling curves for text, image, speech&quot; style=&quot;width:100%; height:auto;&quot; /&gt;
&lt;/div&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;It has been argued that LLMs trained only on text can form &lt;a href=&quot;https://bmk.sh/2021/06/02/Thoughts-on-the-Alignment-Implications-of-Scaling-Language-Models/&quot;&gt;world models for proto-AGI&lt;/a&gt;. These compression results reinforce that claim: despite text-only pretraining, LLMs compress images and speech following clear power laws.&lt;/p&gt;

&lt;p&gt;We hypothesize two primary mechanisms responsible for this:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;In-Context Learning (ICL)&lt;/strong&gt;&lt;br /&gt;
Within a given token window, self-attention adapts on-the-fly to file-specific patterns (repeating pixels, periodic audio cycles), shaving off most of the entropy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Universal Sequence Prior (USP)&lt;/strong&gt;&lt;br /&gt;
Pretraining internalizes bursty repetitions, Zipf’s law, heavy tails, and long-range autocorrelations—statistics shared by all byte streams. Even without context, compression ratio should be far below uniform-noise rates.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Future work should quantify each component’s contribution and trace how they emerge during pretraining.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Kaplan, J.; McCandlish, S.; Henighan, T.; Brown, T.; Chess, B.; Child, R.; Gray, S.; Radford, A.; Wu, J.; Amodei, D. “Scaling Laws for Neural Language Models.” &lt;em&gt;arXiv&lt;/em&gt; 2001.08361 (2020). &lt;a href=&quot;https://arxiv.org/abs/2001.08361&quot;&gt;https://arxiv.org/abs/2001.08361&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;The Intricate Link Between Compression and Prediction. Mindful Modeler Substack (2024). &lt;a href=&quot;https://mindfulmodeler.substack.com/p/the-intricate-link-between-compression&quot;&gt;https://mindfulmodeler.substack.com/p/the-intricate-link-between-compression&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Delétang, G.; Ruoss, A.; Duquenne, P.-A.; Catt, E.; Genewein, T.; Mattern, C.; Grau-Moya, J.; Wenliang, L.K.; Aitchison, M.; Orseau, L.; Hutter, M.; Veness, J. “Language Modeling Is Compression.” &lt;em&gt;arXiv&lt;/em&gt; 2309.10668 (2023). &lt;a href=&quot;https://arxiv.org/abs/2309.10668&quot;&gt;https://arxiv.org/abs/2309.10668&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Gao, Leo. “Thoughts on the Alignment Implications of Scaling Language Models.” BMK.sh Blog (2021). &lt;a href=&quot;https://bmk.sh/2021/06/02/Thoughts-on-the-Alignment-Implications-of-Scaling-Language-Models/&quot;&gt;https://bmk.sh/2021/06/02/Thoughts-on-the-Alignment-Implications-of-Scaling-Language-Models/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Tao, T. “Benford’s Law, Zipf’s Law and the Pareto Distribution.” Terence Tao’s Blog (2009). &lt;a href=&quot;https://terrytao.wordpress.com/2009/07/03/benfords-law-zipfs-law-and-the-pareto-distribution/&quot;&gt;https://terrytao.wordpress.com/2009/07/03/benfords-law-zipfs-law-and-the-pareto-distribution/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Wed, 23 Jul 2025 15:30:00 +0530</pubDate>
        <link>http://localhost:4000/2025/07/23/scaling-compression/</link>
        <guid isPermaLink="true">http://localhost:4000/2025/07/23/scaling-compression/</guid>
        
        
      </item>
    
      <item>
        <title>The God&apos;s Song</title>
        <description>&lt;p&gt;They lit fires in the woods they cleared, and with fire came worship. Old people turned inward, toward stillness, toward silence.&lt;/p&gt;

&lt;p&gt;Wars broke. Races mingled. Gods were shared in smoky air. Some brought rituals, others talked about the soul. And a ladder was built. Those closest to fire rose, those mixed with earth sowed.&lt;/p&gt;

&lt;p&gt;Rituals deepened, but work did not stop. After each fire, they asked: “What is the point of it all?” These questions became forest wisdom, echoes in quiet minds.&lt;/p&gt;

&lt;p&gt;Knowledge passed, but only to some. The rest were needed, to keep the wheels turning, to keep the fire alive.&lt;/p&gt;

&lt;p&gt;The ones atop became scribes. They listened to bards, wove myths into lines, and slowly, a religion grew.&lt;/p&gt;

&lt;p&gt;Centuries passed. Ascetics multiplied. Top to bottom, silence spread.&lt;/p&gt;

&lt;p&gt;Then came the kid from the south, fire in mind, not bound to rite nor retreat. He debated all, rose swift, found the land’s greatest tale: the grand war.&lt;/p&gt;

&lt;p&gt;In its heart, the great dark warrior, in trance, speaking of inaction, yet exalting action. One mind in all forms, he said. Use them fully, there is no better thing.&lt;/p&gt;

&lt;p&gt;The kid was moved. He wrote as none before. “This is the truth,” he said “All is one, but action must go on.”&lt;/p&gt;

&lt;p&gt;And so ends the god’s song.&lt;/p&gt;
</description>
        <pubDate>Tue, 10 Jun 2025 17:49:00 +0530</pubDate>
        <link>http://localhost:4000/2025/06/10/on-gita/</link>
        <guid isPermaLink="true">http://localhost:4000/2025/06/10/on-gita/</guid>
        
        
      </item>
    
      <item>
        <title>Experiments with the Platonic Representation Hypothesis</title>
        <description>&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;https://arxiv.org/pdf/2405.07987&quot;&gt;Platonic Representation Hypothesis&lt;/a&gt; (PRH) suggests that neural networks, despite being trained with different objectives and on various modalities, converge to a shared statistical understanding of reality. Inspired by Plato’s Allegory of the Cave, PRH posits that models across domains—such as vision and language—internalize similar structures, reflecting an idealized model of the world. While initial experiments focused on image-based models like Vision Transformers (ViT) trained on the same pretraining dataset (ImageNet), this raises an important question:&lt;/p&gt;

&lt;h3 id=&quot;does-prh-hold-only-when-models-are-trained-on-data-from-the-same-distribution&quot;&gt;Does PRH hold only when models are trained on data from the same distribution?&lt;/h3&gt;

&lt;p&gt;To evaluate PRH beyond in-distribution data, I extended experiments to three scenarios:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;In-distribution data:&lt;/strong&gt; Utilized the validation set from Places365, aligning closely with the training data of many vision models.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Out-of-distribution (OOD) data:&lt;/strong&gt; Employed the &lt;a href=&quot;https://arxiv.org/pdf/1907.07174&quot;&gt;ImageNet-O&lt;/a&gt; dataset, containing images that diverge significantly from the standard ImageNet distribution.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Random noise:&lt;/strong&gt; Generated purely random images to probe the extreme boundaries of representational alignment.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;results&quot;&gt;Results:&lt;/h3&gt;

&lt;h4 id=&quot;in-distribution-setting&quot;&gt;In-Distribution Setting&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/prh_correlation_plot.png&quot; alt=&quot;Spearman rank correlation between models on Places365&apos;s validation data&quot; style=&quot;width:100%; height:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1: Spearman rank correlation between models on Places365’s validation data, indicating a high degree of alignment in in-distribution data.&lt;/em&gt; This graph is straight from the platonic-representation-hypothesis paper. The ViT models used in this experiment are trained more or less on the same data distribution/pretraining datasets.&lt;/p&gt;

&lt;h4 id=&quot;out-of-distribution-data&quot;&gt;Out-of-Distribution Data&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/spearman_correlation_ood_prh.png&quot; alt=&quot;Alignment of vision models on the ImageNet-O dataset&quot; style=&quot;width:100%; height:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;*Figure 2: Alignment of vision models on the ImageNet-O dataset. Spearman correlation suggests that even with OOD data, models maintain a shared statistical representation, models converge in their representations, implying PRH still holds true and generalizes well outside the training distribution of the model’s training set. This fact has really good implications in understanding failure models and alignment failures of models belonging to same architectural family&lt;/p&gt;

&lt;h4 id=&quot;random-noise-data&quot;&gt;Random Noise Data&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/prh_correlation_rnd.png&quot; alt=&quot;Spearman correlation of alignment scores on random noise data&quot; style=&quot;width:80%; height:auto; display: block; margin-left: auto; margin-right: auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 3: Spearman correlation of alignment scores on random noise data. The lower correlation values indicate a breakdown in representational alignment, suggesting that random data lacks the underlying structure required for PRH.&lt;/em&gt; This means that PRH is not just a random occurance but requires some statistical-structure to be present.&lt;/p&gt;

&lt;h3 id=&quot;discussion&quot;&gt;Discussion&lt;/h3&gt;

&lt;p&gt;When I first came across the PRH paper, my initial hypothesis was that PRH might be primarily due to the high similarity in training datasets, since most LLMs/LVMs are trained on extremely large datasets from common sources (ImageNet, Pile, etc.). However, the OOD experiments proved that this phenomenon of shared statistical structure generalizes well beyond the training distribution of the models. Further experiments with purely random data demonstrated that this is not just a coincidence but rather reflects inherent statistical structures present in “useful” data across all modalities.&lt;/p&gt;

&lt;p&gt;The code for experiments and analysis in this paper is available on GitHub &lt;a href=&quot;https://github.com/rokosbasilisk/prh-experiments&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Sun, 27 Oct 2024 15:30:00 +0530</pubDate>
        <link>http://localhost:4000/2024/10/27/prh-ood/</link>
        <guid isPermaLink="true">http://localhost:4000/2024/10/27/prh-ood/</guid>
        
        
      </item>
    
      <item>
        <title>Understanding Hidden Computations in Chain-of-Thought Reasoning</title>
        <description>&lt;p&gt;Recent research has revealed that transformer models can maintain their reasoning capabilities even when Chain-of-Thought (COT) prompting steps are replaced with filler characters. This post presents my investigation into the mechanisms behind these hidden computations, using the 3SUM task as an experimental framework.&lt;/p&gt;

&lt;h3 id=&quot;background&quot;&gt;Background:&lt;/h3&gt;

&lt;p&gt;Chain-of-Thought prompting has emerged as an effective method for enabling language models to perform complex reasoning tasks. The technique typically involves generating intermediate computational steps before producing a final answer. However, studies have shown that replacing these intermediate steps with filler tokens (e.g., “…”) does not significantly impact model performance.&lt;/p&gt;

&lt;p&gt;To investigate this phenomenon, we focused on the 3SUM task - identifying three numbers in a set that sum to zero. While conceptually straightforward, this task serves as an effective proxy for studying more complex reasoning processes.&lt;/p&gt;

&lt;h3 id=&quot;experimental-setup&quot;&gt;Experimental Setup:&lt;/h3&gt;

&lt;p&gt;My analysis used a 34M parameter LLaMA model with the following specifications:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;4 layers&lt;/li&gt;
  &lt;li&gt;384 hidden dimension&lt;/li&gt;
  &lt;li&gt;6 attention heads&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The model was trained on hidden COT sequences for the 3SUM task, allowing us to analyze how it processes and utilizes filler tokens during reasoning.&lt;/p&gt;

&lt;h3 id=&quot;layer-wise-analysis&quot;&gt;Layer-wise Analysis:&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/hidden_tokens_percentage_by_layer.png&quot; alt=&quot;Percentage comparison of filler decoded tokens&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;My analysis revealed distinct patterns in how representations evolve across the model’s layers:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Initial layers primarily process raw numerical sequences&lt;/li&gt;
  &lt;li&gt;Filler tokens emerge prominently from the third layer&lt;/li&gt;
  &lt;li&gt;Final layers show significant reliance on filler token representations&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This progression suggests a systematic transformation of computational representations across the model’s architecture.&lt;/p&gt;

&lt;h3 id=&quot;token-ranking-analysis&quot;&gt;Token Ranking Analysis:&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/token_comparison_percentages.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Examination of token probabilities revealed consistent patterns:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Filler characters consistently emerged as the highest-ranked tokens&lt;/li&gt;
  &lt;li&gt;Original, non-filler COT sequences remained present in lower-ranked positions&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This finding supports the hypothesis that the model maintains original computational paths while using filler tokens as an overlay.&lt;/p&gt;

&lt;h3 id=&quot;modified-greedy-decoding&quot;&gt;Modified Greedy Decoding:&lt;/h3&gt;

&lt;p&gt;We developed an enhanced decoding algorithm that:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Implements standard greedy decoding&lt;/li&gt;
  &lt;li&gt;Substitutes the second-highest probability token when encountering fillers&lt;/li&gt;
  &lt;li&gt;Maintains this substitution throughout the sequence&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This approach achieved 100% consistency with non-filler COT results on the 3SUM task, demonstrating that the underlying computation remains intact despite the presence of filler tokens.&lt;/p&gt;

&lt;h3 id=&quot;future-work&quot;&gt;Future Work:&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Development of more sophisticated decoding methods&lt;/li&gt;
  &lt;li&gt;Extension to natural language reasoning tasks&lt;/li&gt;
  &lt;li&gt;Enhancement of token hiding techniques beyond single filler tokens&lt;/li&gt;
  &lt;li&gt;Identification of specific computational circuits involved in token hiding&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion:&lt;/h3&gt;

&lt;p&gt;This investigation provides concrete evidence for how transformer models encode and process information in hidden COT sequences. The results demonstrate that computational paths remain intact even when obscured by filler tokens, suggesting new approaches for understanding and interpreting chain-of-thought reasoning in language models.&lt;/p&gt;

&lt;p&gt;The code used for the experiments and analysis is available on GitHub: &lt;a href=&quot;https://github.com/rokosbasilisk/filler_tokens/tree/v2&quot;&gt;https://github.com/rokosbasilisk/filler_tokens/tree/v2&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;appendix-layerwise-view-of-sequences-generated-via-various-decoding-methods&quot;&gt;Appendix: Layerwise View of Sequences Generated via Various Decoding Methods&lt;/h3&gt;

&lt;p&gt;To provide a more detailed look at the results, we’ve included visualizations of the sequences generated by different decoding methods across the model’s layers.&lt;/p&gt;

&lt;h4 id=&quot;1-greedy-decoding&quot;&gt;1. Greedy Decoding:&lt;/h4&gt;
&lt;p&gt;This plot shows the sequences generated by standard greedy decoding across different layers of the model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/greedy_decoding.png&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;2-greedy-decoding-with-rank-2-tokens&quot;&gt;2. Greedy Decoding with Rank-2 Tokens:&lt;/h4&gt;
&lt;p&gt;Here, we visualize the sequences generated when we replace the top-ranked token (usually the hidden character) with the second-highest probability token.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/our_method_decoding.png&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;3-greedy-decoding-with-hidden-tokens-replaced-by-rank-2-tokens-my-method&quot;&gt;3. Greedy Decoding with Hidden Tokens Replaced by Rank-2 Tokens (My Method):&lt;/h4&gt;
&lt;p&gt;This plot demonstrates the proposed method, where we perform greedy decoding but replace filler tokens with the second-highest probability token.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/rank2_decoding.png&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;4-greedy-decoding-with-hidden-tokens-replaced-by-randomly-selected-tokens&quot;&gt;4. Greedy Decoding with Hidden Tokens Replaced by Randomly Selected Tokens:&lt;/h4&gt;
&lt;p&gt;For comparison, this plot shows what happens when we replace filler tokens with randomly selected tokens instead of using the rank-2 tokens.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/random_tokens_decoding.png&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References:&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Pfau, J., Merrill, W., &amp;amp; Bowman, S. R. (2023). Let’s Think Dot by Dot: Hidden Computation in Transformer Language Models. &lt;a href=&quot;https://arxiv.org/abs/2404.15758&quot;&gt;arXiv:2404.15758&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Wei, J., Wang, X., Schuurmans, D., et al. (2022). Chain-of-thought prompting elicits reasoning in large language models. &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;&gt;arXiv:2201.11903&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;nostalgebraist (2020). interpreting GPT: the logit lens &lt;a href=&quot;https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/&quot;&gt;LessWrong post&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Touvron, H., Lavril, T., Izacard, G., et al. (2023). LLaMA: Open and Efficient Foundation Language Models. &lt;a href=&quot;https://arxiv.org/abs/2302.13971&quot;&gt;arXiv:2302.13971&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 28 Aug 2024 07:19:00 +0530</pubDate>
        <link>http://localhost:4000/2024/08/28/fillertokens/</link>
        <guid isPermaLink="true">http://localhost:4000/2024/08/28/fillertokens/</guid>
        
        
      </item>
    
      <item>
        <title>Goal-misgeneralization might be ELK-hard</title>
        <description>&lt;p&gt;Goal misgeneralization and the ELK (Eliciting Latent Knowledge) problem are two of the most intriguing yet well-defined sub-problems within the broader AI alignment challenge. In this post, I’ll share my opinion on why solving goal misgeneralization is as challenging as solving the ELK problem.&lt;/p&gt;

&lt;h3 id=&quot;goal-misgeneralization&quot;&gt;Goal Misgeneralization&lt;/h3&gt;

&lt;p&gt;Goal misgeneralization occurs when a reinforcement learning agent retains its capabilities out-of-distribution yet pursues the wrong goal. This misalignment between the intended goal and the agent’s actual behavior can lead to unexpected and potentially harmful outcomes.
A more detailed explanation of this can be found in the &lt;a href=&quot;https://arxiv.org/abs/2105.14111&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;the-elk-problem&quot;&gt;The ELK Problem&lt;/h3&gt;

&lt;p&gt;Alignment Research Center (ARC) provides a simple, informal explanation of the ELK problem:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Suppose we train a model to predict what the future will look like according to cameras and other sensors. We then use planning algorithms to find a sequence of actions that lead to predicted futures that look good to us.&lt;/p&gt;

  &lt;p&gt;But some action sequences could tamper with the cameras so they show happy humans regardless of what’s really happening. More generally, some futures look great on camera but are actually catastrophically bad.&lt;/p&gt;

  &lt;p&gt;In these cases, the prediction model “knows” facts (like “the camera was tampered with”) that are not visible on camera but would change our evaluation of the predicted future if we learned them. How can we train this model to report its latent knowledge of off-screen events?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A more sophisticated explanation of the ELK problem can be found in the &lt;a href=&quot;https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/&quot;&gt;ELK problem document&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As of now, this problem too remains unsolved.&lt;/p&gt;

&lt;h3 id=&quot;why-solving-goal-misgeneralization-is-as-hard-as-solving-elk&quot;&gt;Why Solving Goal Misgeneralization is as Hard as Solving ELK&lt;/h3&gt;

&lt;p&gt;Consider an adversarial training scheme for solving goal misgeneralization, such as Redwood Research’s work on &lt;a href=&quot;https://arxiv.org/pdf/2205.01663.pdf&quot;&gt;“Adversarial Training for High-Stakes Reliability”&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Let’s examine a model trained to perform a specific task. To guarantee worst-case performance for this model, we need to establish bounds for its outputs in adversarial examples. However, running the model in the actual environment is not feasible due to potential unwanted consequences.&lt;/p&gt;

&lt;p&gt;To address this, we can use a prediction-head to generate a simulated version of the trajectory that the model might undergo. We can then train a classifier from the output of the prediction-head to determine whether the AI system exhibits goal misgeneralization in an adversarial manner. The model can be penalized for misgeneralization behavior, and the resulting policy can be iteratively distilled into a newer model.&lt;/p&gt;

&lt;p&gt;This prediction-head can be used to build another model that acts as a classifier, allowing us to distill the conservative policy (model) into a newer one.&lt;/p&gt;

&lt;p&gt;However, this approach is only viable when the prediction-head is not deceiving, which is essentially the same challenge posed by the ELK problem.&lt;/p&gt;

&lt;p&gt;In conclusion, the difficulty of solving goal misgeneralization for the worst case mirrors the core challenge of the ELK problem.&lt;/p&gt;
</description>
        <pubDate>Fri, 24 Mar 2023 15:30:00 +0530</pubDate>
        <link>http://localhost:4000/2023/03/24/goalmisgen/</link>
        <guid isPermaLink="true">http://localhost:4000/2023/03/24/goalmisgen/</guid>
        
        
      </item>
    
      <item>
        <title>The AGI needs to be honest</title>
        <description>&lt;p&gt;Imagine that you are a trained mathematician and you have been assigned the job of testing an arbitrarily intelligent chatbot for its intelligence.&lt;/p&gt;

&lt;p&gt;You being knowledgeable about a fair amount of computer-science theory won’t test it with the likes of Turing-test or similar, since such a bot might have any useful priors about the world.&lt;/p&gt;

&lt;p&gt;You have asked it find a proof for the “Riemann hypothesis”. the bot started its search program and after several months it gave you gigantic proof written in coq.&lt;/p&gt;

&lt;p&gt;You have tried to run the proof through a proof-checking assistant but quickly realized that checking that itself would years or decades, also no other computer except the one running the bot is sophisticated enough to run such a gigantic proof.&lt;/p&gt;

&lt;p&gt;You have asked the bot to provide you a zero-knowledge-proof, but being a trained mathematician you know that a zero-knowledge-proof of sufficient credibility requires as much compute as the original one. also, the correctness is directly linked to the length of the proof it generates.&lt;/p&gt;

&lt;p&gt;You know that the bot may have formed increasingly complex abstractions while solving the problem, and it would be very hard to describe those in exact language to you.&lt;/p&gt;

&lt;p&gt;You have asked the bot to summarize the proof for you in natural-language, but you know that the bot can easily trick you into accepting the proof.&lt;/p&gt;

&lt;p&gt;You have now started to think about a bigger question, the bot essentially is a powerful optimizer. In this case, the bot is trained to find proofs, its reward is based on finding what a group of mathematicians agree on how a correct proof looks like.&lt;/p&gt;

&lt;p&gt;But the bot being bot doesn’t care about being true to you or itself, it is not rewarded for being “honest” it is only being rewarded for finding proof-like strings that humans may select or reject.&lt;/p&gt;

&lt;p&gt;So it is far easier to find a large coq-program, large enough that you cannot check by any other means than to find a proof for Riemann-hypothesis.&lt;/p&gt;

&lt;p&gt;Now you have concluded that before you certify that the bot is intelligent, you have to prove that the bot is being honest.&lt;/p&gt;

&lt;p&gt;Going by the current trend, it is okay for us to assume that such an arbitrarily intelligent bot would have a significant part of it based on the principles of the current deep-learning stack. assume it be a large neural-network-based agent, also assume that the language-understanding component is somewhat based on the current language-model design.&lt;/p&gt;

&lt;p&gt;So how do you know that the large language model is being honest?&lt;/p&gt;

&lt;p&gt;A quick look at the plots of results on truthful-qa dataset shows that truthfulness reduces with the model-size, going by this momentum any large-models trained on large datasets are more likely to give fluke answers to significantly complex questions.
&lt;img src=&quot;/assets/truthfulqa.png&quot; alt=&quot;*TruthfulQA benchmark*&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Any significantly complex decision-question if cast into an optimization problem has one hard-to-find global-minima called “truth” but an extremely large number of easy-to-find local-minima, how do you then make a powerful optimizer optimize for honesty?&lt;/p&gt;

</description>
        <pubDate>Sat, 16 Oct 2021 15:30:00 +0530</pubDate>
        <link>http://localhost:4000/2021/10/16/truth/</link>
        <guid isPermaLink="true">http://localhost:4000/2021/10/16/truth/</guid>
        
        
      </item>
    
  </channel>
</rss>
