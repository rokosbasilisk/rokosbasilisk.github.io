<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ram Bharadwaj</title>
    <description>cached thoughts</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sun, 01 Sep 2024 14:03:21 +0530</pubDate>
    <lastBuildDate>Sun, 01 Sep 2024 14:03:21 +0530</lastBuildDate>
    <generator>Jekyll v3.8.6</generator>
    
      <item>
        <title>Understanding Hidden Computations in Chain-of-Thought Reasoning</title>
        <description>&lt;p&gt;Recent work has demonstrated that transformer models can perform complex reasoning tasks using Chain-of-Thought (COT) prompting, even when the COT is replaced with filler characters. This post summarizes our investigation into methods for decoding these hidden computations, focusing on the 3SUM task.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Chain-of-Thought (COT) Prompting&lt;/strong&gt;: A technique that improves the performance of large language models on complex reasoning tasks by eliciting intermediate steps [1].&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;COT using filler tokens&lt;/strong&gt;: Replacing intermediate reasoning steps with filler characters (e.g., “…”) while maintaining model performance [2].&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;3SUM Task&lt;/strong&gt;: A problem requiring the identification of three numbers in a set that sum to zero, (here as a proxy for more complex reasoning tasks).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Methodology&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We analyzed a 34M parameter LLaMA model with 4 layers, 384 hidden dimension, and 6 attention heads, this setup is same as mentioned in [2], trained on hidden COT (COT using filler tokens)  sequences for the 3SUM task. Our analysis focused on three main areas:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Layer-wise Representation Analysis&lt;/li&gt;
  &lt;li&gt;Token Ranking&lt;/li&gt;
  &lt;li&gt;Modified Greedy Decoding Algorithm&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Layer-wise Analysis:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/hidden_tokens_percentage_by_layer.png&quot; alt=&quot;Percentage comparison of filler decoded tokens&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our analysis revealed a gradual evolution of representations across the model’s layers:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Initial layers: Primarily raw numerical sequences&lt;/li&gt;
  &lt;li&gt;Third layer onwards: Emergence of filler tokens&lt;/li&gt;
  &lt;li&gt;Final layers: Extensive reliance on filler tokens&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This suggests the model develops the ability to use filler tokens as proxies in its deeper layers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Token Rank Analysis:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Top-ranked token: Consistently the filler character (“.”)&lt;/li&gt;
  &lt;li&gt;Lower-ranked tokens: Revealed the original, non-filler COT sequences&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This supports the hypothesis that the model replaces computation with filler tokens while keeping the original computation intact underneath.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Modified Greedy Decoding Algorithm:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/token_comparison_percentages.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We implemented a modified greedy autoregressive decoding method:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Perform standard greedy decoding&lt;/li&gt;
  &lt;li&gt;Select the second-highest probability token when encountering a filler token&lt;/li&gt;
  &lt;li&gt;Continue this process for the entire sequence&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This method resulted in a 100% match in 3SUM task results with and without filler tokens. I.e, replacing the filler tokens with the rank-2 tokens do not affect the performance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Future Work:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Potential for developing better decoding methods or identifying circuits that hide tokens&lt;/li&gt;
  &lt;li&gt;Investigating generalizability to tasks beyond 3SUM, including natural language tasks&lt;/li&gt;
  &lt;li&gt;Improving token hiding methods (currently limited to one filler token)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Conclusion:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Our approach to understanding hidden computations in transformer models through token ranking analysis provides new insights into how models encode and process information in filler/hidden COT sequences. This work opens avenues for interpreting chain-of-thought reasoning in language models.&lt;/p&gt;

&lt;p&gt;The code used for the experiments and analysis is available on GitHub: &lt;a href=&quot;https://github.com/rokosbasilisk/filler_tokens/tree/v2&quot;&gt;https://github.com/rokosbasilisk/filler_tokens/tree/v2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Appendix:&lt;/strong&gt; Layerwise View of Sequences Generated via Various Decoding Methods
To provide a more detailed look at our results, we’ve included visualizations of the sequences generated by different decoding methods across the model’s layers.&lt;/p&gt;

&lt;h1 id=&quot;1-greedy-decoding&quot;&gt;1. Greedy Decoding&lt;/h1&gt;
&lt;p&gt;This plot shows the sequences generated by standard greedy decoding across different layers of the model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/greedy_decoding.png&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;2-greedy-decoding-with-rank-2-tokens&quot;&gt;2. Greedy Decoding with Rank-2 Tokens&lt;/h1&gt;
&lt;p&gt;Here, we visualize the sequences generated when we replace the top-ranked token (usually the hidden character) with the second-highest probability token.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/our_method_decoding.png&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;3-our-method-greedy-decoding-with-hidden-tokens-replaced-by-rank-2-tokens&quot;&gt;3. Our Method: Greedy Decoding with Hidden Tokens Replaced by Rank-2 Tokens&lt;/h1&gt;
&lt;p&gt;This plot demonstrates our proposed method, where we perform greedy decoding but replace filler tokens with the second-highest probability token.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/rank2_decoding.png&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;4-greedy-decoding-with-hidden-tokens-replaced-by-randomly-selected-tokens&quot;&gt;4. Greedy Decoding with Hidden Tokens Replaced by Randomly Selected Tokens&lt;/h1&gt;
&lt;p&gt;For comparison, this plot shows what happens when we replace filler tokens with randomly selected tokens instead of using the rank-2 tokens.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/random_tokens_decoding.png&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Pfau, J., Merrill, W., &amp;amp; Bowman, S. R. (2023). Let’s Think Dot by Dot: Hidden Computation in Transformer Language Models. &lt;a href=&quot;https://arxiv.org/abs/2404.15758&quot;&gt;arXiv:2404.15758&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Wei, J., Wang, X., Schuurmans, D., et al. (2022). Chain-of-thought prompting elicits reasoning in large language models. &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;&gt;arXiv:2201.11903&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;nostalgebraist (2020). interpreting GPT: the logit lens &lt;a href=&quot;https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/&quot;&gt;LessWrong post&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Touvron, H., Lavril, T., Izacard, G., et al. (2023). LLaMA: Open and Efficient Foundation Language Models. &lt;a href=&quot;https://arxiv.org/abs/2302.13971&quot;&gt;arXiv:2302.13971&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 28 Aug 2024 07:19:00 +0530</pubDate>
        <link>http://localhost:4000/2024/08/28/fillertokens/</link>
        <guid isPermaLink="true">http://localhost:4000/2024/08/28/fillertokens/</guid>
        
        
      </item>
    
      <item>
        <title>Goal-misgeneralization is ELK-hard</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://www.lesswrong.com/posts/MWSCqzPrAbNrYoqWv/goal-misgeneralization-is-elk-hard&quot;&gt;&lt;em&gt;link to lesswrong post&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Consider an adversarial training-scheme for solving goal-misgeneralization, ( here i consider Redwood Research’s work on &lt;a href=&quot;https://arxiv.org/pdf/2205.01663.pdf&quot;&gt;“Adversarial Training for High-Stakes Reliability”&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Consider a model that was trained to perform a specific task. To guarantee worst-case performance for this model, we need to have bounds for its outputs in adversarial examples. However, it is not feasible to run the model in the actual environment because it could lead to unwanted consequences.&lt;/p&gt;

&lt;p&gt;Therefore, we can use a prediction-head to generate a simulated version of the trajectory that the model might undergo. We can train a classifier from the output of the prediction-head to classify whether the AI system exhibits goal-misgeneralization in an adversarial manner. The model can then be penalized for misgeneralization behavior, and the policy obtained can be distilled into a newer model iteratively. We can use this prediction-head to build another model that can act as a classifier and distill the conservative policy (model) into a newer one.&lt;/p&gt;

&lt;p&gt;However, this is only possible when the prediction-head is not deceiving, which is essentially the same as the &lt;a href=&quot;https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/&quot;&gt;ELK problem&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Fri, 24 Mar 2023 15:30:00 +0530</pubDate>
        <link>http://localhost:4000/2023/03/24/goalmisgen/</link>
        <guid isPermaLink="true">http://localhost:4000/2023/03/24/goalmisgen/</guid>
        
        
      </item>
    
      <item>
        <title>The AGI needs to be honest</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://www.lesswrong.com/posts/hqzHbew35Jx4xoDhE/the-agi-needs-to-be-honest&quot;&gt;&lt;em&gt;link to lesswrong post&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Imagine that you are a trained mathematician and you have been assigned the job of testing an arbitrarily intelligent chatbot for its intelligence.&lt;/p&gt;

&lt;p&gt;You being knowledgeable about a fair amount of computer-science theory won’t test it with the likes of Turing-test or similar, since such a bot might have any useful priors about the world.&lt;/p&gt;

&lt;p&gt;You have asked it find a proof for the “Riemann hypothesis”. the bot started its search program and after several months it gave you gigantic proof written in coq.&lt;/p&gt;

&lt;p&gt;You have tried to run the proof through a proof-checking assistant but quickly realized that checking that itself would years or decades, also no other computer except the one running the bot is sophisticated enough to run such a gigantic proof.&lt;/p&gt;

&lt;p&gt;You have asked the bot to provide you a zero-knowledge-proof, but being a trained mathematician you know that a zero-knowledge-proof of sufficient credibility requires as much compute as the original one. also, the correctness is directly linked to the length of the proof it generates.&lt;/p&gt;

&lt;p&gt;You know that the bot may have formed increasingly complex abstractions while solving the problem, and it would be very hard to describe those in exact language to you.&lt;/p&gt;

&lt;p&gt;You have asked the bot to summarize the proof for you in natural-language, but you know that the bot can easily trick you into accepting the proof.&lt;/p&gt;

&lt;p&gt;You have now started to think about a bigger question, the bot essentially is a powerful optimizer. In this case, the bot is trained to find proofs, its reward is based on finding what a group of mathematicians agree on how a correct proof looks like.&lt;/p&gt;

&lt;p&gt;But the bot being bot doesn’t care about being true to you or itself, it is not rewarded for being “honest” it is only being rewarded for finding proof-like strings that humans may select or reject.&lt;/p&gt;

&lt;p&gt;So it is far easier to find a large coq-program, large enough that you cannot check by any other means than to find a proof for Riemann-hypothesis.&lt;/p&gt;

&lt;p&gt;Now you have concluded that before you certify that the bot is intelligent, you have to prove that the bot is being honest.&lt;/p&gt;

&lt;p&gt;Going by the current trend, it is okay for us to assume that such an arbitrarily intelligent bot would have a significant part of it based on the principles of the current deep-learning stack. assume it be a large neural-network-based agent, also assume that the language-understanding component is somewhat based on the current language-model design.&lt;/p&gt;

&lt;p&gt;So how do you know that the large language model is being honest?&lt;/p&gt;

&lt;p&gt;A quick look at the plots of results on truthful-qa dataset shows that truthfulness reduces with the model-size, going by this momentum any large-models trained on large datasets are more likely to give fluke answers to significantly complex questions.
&lt;img src=&quot;/assets/truthfulqa.png&quot; alt=&quot;*TruthfulQA benchmark*&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Any significantly complex decision-question if cast into an optimization problem has one hard-to-find global-minima called “truth” but an extremely large number of easy-to-find local-minima, how do you then make a powerful optimizer optimize for honesty?&lt;/p&gt;

</description>
        <pubDate>Sat, 16 Oct 2021 15:30:00 +0530</pubDate>
        <link>http://localhost:4000/2021/10/16/truth/</link>
        <guid isPermaLink="true">http://localhost:4000/2021/10/16/truth/</guid>
        
        
      </item>
    
  </channel>
</rss>
