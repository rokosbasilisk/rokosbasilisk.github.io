<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Understanding Hidden Computations in Chain-of-Thought Reasoning</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="">
    <link rel="canonical" href="http://localhost:4000/2024/08/28/fillertokens/">
    <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Full Wrong posts" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">

    <!-- Google Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-3698471-23', 'auto');
      ga('send', 'pageview');
    </script>

</head>


    <body>

    <header class="site-header">

  <div class="wrap">

    <div style="float:left; margin-top:10px; margin-right:10px;">
    </div>

    <a class="site-title" href="/">Full Wrong</a>
    
    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <div class="trigger">
        
          
        
          <a class="page-link" href="/about/">About</a>
        
          
        
          
        
      </div>
    </nav>
  </div>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>Understanding Hidden Computations in Chain-of-Thought Reasoning</h1>
    <p class="meta">Aug 28, 2024</p>
  </header>

  <article class="post-content">
  <p>Recent research has revealed that transformer models can maintain their reasoning capabilities even when Chain-of-Thought (COT) prompting steps are replaced with filler characters. This post presents my investigation into the mechanisms behind these hidden computations, using the 3SUM task as an experimental framework.</p>

<h3 id="background">Background:</h3>

<p>Chain-of-Thought prompting has emerged as an effective method for enabling language models to perform complex reasoning tasks. The technique typically involves generating intermediate computational steps before producing a final answer. However, studies have shown that replacing these intermediate steps with filler tokens (e.g., “…”) does not significantly impact model performance.</p>

<p>To investigate this phenomenon, we focused on the 3SUM task - identifying three numbers in a set that sum to zero. While conceptually straightforward, this task serves as an effective proxy for studying more complex reasoning processes.</p>

<h3 id="experimental-setup">Experimental Setup:</h3>

<p>My analysis used a 34M parameter LLaMA model with the following specifications:</p>
<ul>
  <li>4 layers</li>
  <li>384 hidden dimension</li>
  <li>6 attention heads</li>
</ul>

<p>The model was trained on hidden COT sequences for the 3SUM task, allowing us to analyze how it processes and utilizes filler tokens during reasoning.</p>

<h3 id="layer-wise-analysis">Layer-wise Analysis:</h3>

<p><img src="/assets/hidden_tokens_percentage_by_layer.png" alt="Percentage comparison of filler decoded tokens" width="80%" /></p>

<p>My analysis revealed distinct patterns in how representations evolve across the model’s layers:</p>
<ol>
  <li>Initial layers primarily process raw numerical sequences</li>
  <li>Filler tokens emerge prominently from the third layer</li>
  <li>Final layers show significant reliance on filler token representations</li>
</ol>

<p>This progression suggests a systematic transformation of computational representations across the model’s architecture.</p>

<h3 id="token-ranking-analysis">Token Ranking Analysis:</h3>

<p><img src="/assets/token_comparison_percentages.png" width="80%" /></p>

<p>Examination of token probabilities revealed consistent patterns:</p>
<ul>
  <li>Filler characters consistently emerged as the highest-ranked tokens</li>
  <li>Original, non-filler COT sequences remained present in lower-ranked positions</li>
</ul>

<p>This finding supports the hypothesis that the model maintains original computational paths while using filler tokens as an overlay.</p>

<h3 id="modified-greedy-decoding">Modified Greedy Decoding:</h3>

<p>We developed an enhanced decoding algorithm that:</p>
<ol>
  <li>Implements standard greedy decoding</li>
  <li>Substitutes the second-highest probability token when encountering fillers</li>
  <li>Maintains this substitution throughout the sequence</li>
</ol>

<p>This approach achieved 100% consistency with non-filler COT results on the 3SUM task, demonstrating that the underlying computation remains intact despite the presence of filler tokens.</p>

<h3 id="future-work">Future Work:</h3>

<ol>
  <li>Development of more sophisticated decoding methods</li>
  <li>Extension to natural language reasoning tasks</li>
  <li>Enhancement of token hiding techniques beyond single filler tokens</li>
  <li>Identification of specific computational circuits involved in token hiding</li>
</ol>

<h3 id="conclusion">Conclusion:</h3>

<p>This investigation provides concrete evidence for how transformer models encode and process information in hidden COT sequences. The results demonstrate that computational paths remain intact even when obscured by filler tokens, suggesting new approaches for understanding and interpreting chain-of-thought reasoning in language models.</p>

<p>The code used for the experiments and analysis is available on GitHub: <a href="https://github.com/rokosbasilisk/filler_tokens/tree/v2">https://github.com/rokosbasilisk/filler_tokens/tree/v2</a></p>

<h3 id="appendix-layerwise-view-of-sequences-generated-via-various-decoding-methods">Appendix: Layerwise View of Sequences Generated via Various Decoding Methods</h3>

<p>To provide a more detailed look at the results, we’ve included visualizations of the sequences generated by different decoding methods across the model’s layers.</p>

<h4 id="1-greedy-decoding">1. Greedy Decoding:</h4>
<p>This plot shows the sequences generated by standard greedy decoding across different layers of the model.</p>

<p><img src="/assets/greedy_decoding.png" width="90%" /></p>

<h4 id="2-greedy-decoding-with-rank-2-tokens">2. Greedy Decoding with Rank-2 Tokens:</h4>
<p>Here, we visualize the sequences generated when we replace the top-ranked token (usually the hidden character) with the second-highest probability token.</p>

<p><img src="/assets/our_method_decoding.png" width="90%" /></p>

<h4 id="3-greedy-decoding-with-hidden-tokens-replaced-by-rank-2-tokens-my-method">3. Greedy Decoding with Hidden Tokens Replaced by Rank-2 Tokens (My Method):</h4>
<p>This plot demonstrates the proposed method, where we perform greedy decoding but replace filler tokens with the second-highest probability token.</p>

<p><img src="/assets/rank2_decoding.png" width="90%" /></p>

<h4 id="4-greedy-decoding-with-hidden-tokens-replaced-by-randomly-selected-tokens">4. Greedy Decoding with Hidden Tokens Replaced by Randomly Selected Tokens:</h4>
<p>For comparison, this plot shows what happens when we replace filler tokens with randomly selected tokens instead of using the rank-2 tokens.</p>

<p><img src="/assets/random_tokens_decoding.png" width="90%" /></p>

<h3 id="references">References:</h3>

<ol>
  <li>
    <p>Pfau, J., Merrill, W., &amp; Bowman, S. R. (2023). Let’s Think Dot by Dot: Hidden Computation in Transformer Language Models. <a href="https://arxiv.org/abs/2404.15758">arXiv:2404.15758</a>.</p>
  </li>
  <li>
    <p>Wei, J., Wang, X., Schuurmans, D., et al. (2022). Chain-of-thought prompting elicits reasoning in large language models. <a href="https://arxiv.org/abs/2201.11903">arXiv:2201.11903</a>.</p>
  </li>
  <li>
    <p>nostalgebraist (2020). interpreting GPT: the logit lens <a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/">LessWrong post</a>.</p>
  </li>
  <li>
    <p>Touvron, H., Lavril, T., Izacard, G., et al. (2023). LLaMA: Open and Efficient Foundation Language Models. <a href="https://arxiv.org/abs/2302.13971">arXiv:2302.13971</a>.</p>
  </li>
</ol>

  </article>

  <!-- mathjax -->
  
  
  <!-- disqus comments -->
 
  
</div>
      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <!-- <h2 class="footer-heading">Full Wrong</h2> -->

    <div class="footer-col-1 column">
      <ul>
        <li>Full Wrong</li>
        <!-- <li><a href="mailto:"></a></li> -->
      </ul>
    </div>

    <div class="footer-col-2 column">
      <ul>
        <li>
          <a href="https://github.com/rokosbasilisk">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">rokosbasilisk</span>
          </a>
        </li>
        
      </ul>
    </div>

    <div class="footer-col-3 column">
      <p class="text"></p>
    </div>

  </div>

</footer>


    </body>
</html>